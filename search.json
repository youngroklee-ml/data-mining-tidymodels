[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Data mining with {tidymodels}",
    "section": "",
    "text": "1 Preface\nThis is a book to convert example code on https://youngroklee-ml.github.io/data-mining-techniques/ into the {tidymodels} framework.",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Preface</span>"
    ]
  },
  {
    "objectID": "index.html#tidymodels",
    "href": "index.html#tidymodels",
    "title": "Data mining with {tidymodels}",
    "section": "1.1 Tidymodels",
    "text": "1.1 Tidymodels\nThe tidymodels framework is a collection of packages to provide intuitive and unified interface for modeling and machine learning. To get more information, please see the following materials:\n\ntidymodels website: https://www.tidymodels.org\n“Tidy modeling with R” by Max Kuhn and Julia Silge: https://www.tmwr.org",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Preface</span>"
    ]
  },
  {
    "objectID": "index.html#model-engine-packages",
    "href": "index.html#model-engine-packages",
    "title": "Data mining with {tidymodels}",
    "section": "1.2 Model engine packages",
    "text": "1.2 Model engine packages\nEach modeling packages need to be separately installed to use it within the tidymodels framework. Example codes in this book require the following R packages.\n\nlibrary(glmnet)\nlibrary(mixOmics)",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Preface</span>"
    ]
  },
  {
    "objectID": "ch02_regression.html",
    "href": "ch02_regression.html",
    "title": "2  Regression",
    "section": "",
    "text": "2.1 Examples 2.3 - 2.5, 2.7, 2.10 - 2.11",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Regression</span>"
    ]
  },
  {
    "objectID": "ch02_regression.html#examples-2.3---2.5-2.7-2.10---2.11",
    "href": "ch02_regression.html#examples-2.3---2.5-2.7-2.10---2.11",
    "title": "2  Regression",
    "section": "",
    "text": "2.1.1 Load data\n\ndat1 &lt;- read_csv(\"data/ch2_reg1.csv\")\n\nRows: 10 Columns: 4\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \",\"\ndbl (4): ID, age, height, weight\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n\nprint(dat1)\n\n# A tibble: 10 × 4\n      ID   age height weight\n   &lt;dbl&gt; &lt;dbl&gt;  &lt;dbl&gt;  &lt;dbl&gt;\n 1     1    21    170     60\n 2     2    47    167     65\n 3     3    36    173     67\n 4     4    15    165     54\n 5     5    54    168     73\n 6     6    25    177     71\n 7     7    32    169     68\n 8     8    18    172     62\n 9     9    43    171     66\n10    10    28    175     68\n\n\n\n\n2.1.2 Ex 2.3: Estimate coefficients\nUse {parsnip} package, a part of {tidymodels}, that provide a unified modeling interface.\nDefine a model type and engine.\n\nmodel &lt;- \n  linear_reg() |&gt;\n  set_engine(\"lm\") # \"lm\" is a default engine for `linear_reg()`\n\nEstimate the model by calling fit() function with formula and training data.\n\nmodel_fit &lt;- \n  model |&gt; \n  fit(weight ~ age + height, data = dat1)\n\nLet’s print estimation results.\n\nprint(model_fit)\n\nparsnip model object\n\n\nCall:\nstats::lm(formula = weight ~ age + height, data = data)\n\nCoefficients:\n(Intercept)          age       height  \n  -108.1672       0.3291       0.9553  \n\n\nbroom::tidy() can extract coefficient statistics as data frame, including estimates and test statistics. Column estimate represents the coefficient estimates.\n\ntidy(model_fit)\n\n# A tibble: 3 × 5\n  term        estimate std.error statistic p.value\n  &lt;chr&gt;          &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;   &lt;dbl&gt;\n1 (Intercept) -108.      42.1        -2.57 0.0371 \n2 age            0.329    0.0692      4.75 0.00208\n3 height         0.955    0.244       3.91 0.00579\n\n\n\n\n2.1.3 Ex 2.4: Estimate variance of error term\nbroom::glance() provides model-level statistics.\n\nglance(model_fit)\n\n# A tibble: 1 × 12\n  r.squared adj.r.squared sigma statistic p.value    df logLik   AIC   BIC\n      &lt;dbl&gt;         &lt;dbl&gt; &lt;dbl&gt;     &lt;dbl&gt;   &lt;dbl&gt; &lt;dbl&gt;  &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;\n1     0.822         0.771  2.65      16.1 0.00239     2  -22.2  52.3  53.5\n# ℹ 3 more variables: deviance &lt;dbl&gt;, df.residual &lt;int&gt;, nobs &lt;int&gt;\n\n\nsigma is the estimate of standard deviation of the error term, so just square it to estimate the variance of the error term.\n\nglance(model_fit)[[\"sigma\"]]^2\n\n[1] 7.038464\n\n\n\n\n2.1.4 Ex 2.5: Test a model\nCall extract_fit_engine() when you need to explicitly use underlying lm object.\n\nextract_fit_engine(model_fit)\n\n\nCall:\nstats::lm(formula = weight ~ age + height, data = data)\n\nCoefficients:\n(Intercept)          age       height  \n  -108.1672       0.3291       0.9553  \n\n\nThis is important when a function that you call require the underlying engine’s object, not tidymodels framework’s wrapper class. anova() function to conduct ANOVA test is one of such functions that you need to pass lm object.\n\nmodel_fit |&gt; \n  extract_fit_engine() |&gt; \n  anova()\n\nAnalysis of Variance Table\n\nResponse: weight\n          Df  Sum Sq Mean Sq F value   Pr(&gt;F)   \nage        1 119.299 119.299   16.95 0.004476 **\nheight     1 107.831 107.831   15.32 0.005793 **\nResiduals  7  49.269   7.038                    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\n\n\n2.1.5 Ex 2.7: Test each coefficient\nAs seen above, tidy() function returns data frame that contain test statistics.\n\ntidy(model_fit)\n\n# A tibble: 3 × 5\n  term        estimate std.error statistic p.value\n  &lt;chr&gt;          &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;   &lt;dbl&gt;\n1 (Intercept) -108.      42.1        -2.57 0.0371 \n2 age            0.329    0.0692      4.75 0.00208\n3 height         0.955    0.244       3.91 0.00579\n\n\n\n\n2.1.6 Ex 2.10: Mean prediciton\nLet’s create a new data set as a tidyverse data frame tibble.\n\nnewdata &lt;- tibble(age = 40, height = 170)\n\nMake a mean prediction by calling predict() function, and append it to the data set as a column by calling bind_cols().\n\nnewdata |&gt; \n  bind_cols(predict(model_fit, new_data = newdata))\n\n# A tibble: 1 × 3\n    age height .pred\n  &lt;dbl&gt;  &lt;dbl&gt; &lt;dbl&gt;\n1    40    170  67.4\n\n\nAlso, add 95% confidence interval by calling predict() function with argument type = \"conf_int\".\n\nnewdata |&gt; \n  bind_cols(predict(model_fit, new_data = newdata)) |&gt; \n  bind_cols(predict(model_fit, new_data = newdata, type = \"conf_int\"))\n\n# A tibble: 1 × 5\n    age height .pred .pred_lower .pred_upper\n  &lt;dbl&gt;  &lt;dbl&gt; &lt;dbl&gt;       &lt;dbl&gt;       &lt;dbl&gt;\n1    40    170  67.4        65.0        69.8\n\n\n\n\n2.1.7 Ex 2.11 Prediction interval\nInstead of confidence interval, you can add prediction interval by passing argument type = \"pred_int\".\n\nnewdata |&gt; \n  bind_cols(predict(model_fit, new_data = newdata)) |&gt; \n  bind_cols(predict(model_fit, new_data = newdata, type = \"pred_int\"))\n\n# A tibble: 1 × 5\n    age height .pred .pred_lower .pred_upper\n  &lt;dbl&gt;  &lt;dbl&gt; &lt;dbl&gt;       &lt;dbl&gt;       &lt;dbl&gt;\n1    40    170  67.4        60.7        74.1",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Regression</span>"
    ]
  },
  {
    "objectID": "ch02_regression.html#examples-2.14-2.16",
    "href": "ch02_regression.html#examples-2.14-2.16",
    "title": "2  Regression",
    "section": "2.2 Examples 2.14, 2.16",
    "text": "2.2 Examples 2.14, 2.16\n\n2.2.1 Load data\n\ndat1 &lt;- read_csv(\"data/ch2_coil.csv\")\n\nRows: 10 Columns: 3\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \",\"\ndbl (3): temp, thick, y\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n\n\n\n\n2.2.2 Ex 2.14: Indicator variable\n\n2.2.2.1 Set indicator variable\nDefine feature engineering steps with {recipes} package, a part of {tidymodels}.\nFirst, call recipe() to define input variables, output variable, and data.\n\nrec &lt;- \n  recipe(y ~ temp + thick, data = dat1)\n\nSee the recipe recognizes that there are one outcome variable and two predictor variables.\n\nrec\n\n\n\n\n── Recipe ──────────────────────────────────────────────────────────────────────\n\n\n\n\n\n── Inputs \n\n\nNumber of variables by role\n\n\noutcome:   1\npredictor: 2\n\n\nYou want to consider thick as a categorical variable by using factor() inside step_mutate() and create a dummy variable by adding step_dummy().\n\nrec &lt;- \n  rec |&gt; \n  step_mutate(thick = factor(thick, levels = c(6, 2))) |&gt; \n  step_dummy(thick)\n\nNow check that the updated recipe recognize thick as a dummy variable.\n\nrec\n\n\n\n\n── Recipe ──────────────────────────────────────────────────────────────────────\n\n\n\n\n\n── Inputs \n\n\nNumber of variables by role\n\n\noutcome:   1\npredictor: 2\n\n\n\n\n\n── Operations \n\n\n• Variable mutation for: factor(thick, levels = c(6, 2))\n\n\n• Dummy variables from: thick\n\n\nOf course, you can make write it as one pipeline\n\nrec &lt;- \n  recipe(y ~ temp + thick, data = dat1) |&gt; \n  step_mutate(thick = factor(thick, levels = c(6, 2))) |&gt; \n  step_dummy(thick)\n\nQuickly see what the output would look like. Here, prep() is a function to train feature engineering, and juice() is a result of applying the feature engineering to training data.\n\nrec |&gt; \n  prep() |&gt; \n  juice()\n\n# A tibble: 10 × 3\n    temp     y thick_X2\n   &lt;dbl&gt; &lt;dbl&gt;    &lt;dbl&gt;\n 1   540  52.5        1\n 2   660  50.2        1\n 3   610  51.3        1\n 4   710  49.1        1\n 5   570  50.8        0\n 6   700  48.7        0\n 7   560  51.2        0\n 8   600  50.8        0\n 9   680  49.3        0\n10   530  51.5        0\n\n\n\n\n2.2.2.2 Include feature engineering into a workflow\nLet’s start with defining model type and engine as we did in the first example above.\n\nmodel &lt;- \n  linear_reg() |&gt; \n  set_engine(\"lm\")\n\nNow, let’s define a “workflow” from {workflows} package, a part of {tidymodels}, to combine a recipe and a model.\n\nwflow &lt;- \n  workflow() |&gt; \n  add_recipe(rec) |&gt; \n  add_model(model) \n\nThe workflow recognizes that rec is a preprocessor before estimating a regression model.\n\nwflow\n\n══ Workflow ════════════════════════════════════════════════════════════════════\nPreprocessor: Recipe\nModel: linear_reg()\n\n── Preprocessor ────────────────────────────────────────────────────────────────\n2 Recipe Steps\n\n• step_mutate()\n• step_dummy()\n\n── Model ───────────────────────────────────────────────────────────────────────\nLinear Regression Model Specification (regression)\n\nComputational engine: lm \n\n\nNow, call fit() function by passing training data to estimate a regression model.\n\nlm_model &lt;- \n  wflow |&gt; \n  fit(dat1)\n\nSee the model estimation results.\n\nlm_model\n\n══ Workflow [trained] ══════════════════════════════════════════════════════════\nPreprocessor: Recipe\nModel: linear_reg()\n\n── Preprocessor ────────────────────────────────────────────────────────────────\n2 Recipe Steps\n\n• step_mutate()\n• step_dummy()\n\n── Model ───────────────────────────────────────────────────────────────────────\n\nCall:\nstats::lm(formula = ..y ~ ., data = data)\n\nCoefficients:\n(Intercept)         temp     thick_X2  \n   61.10797     -0.01768      0.80415  \n\n\nYou can still use broom::tidy() function to return coefficient-level statistics.\n\ntidy(lm_model)\n\n# A tibble: 3 × 5\n  term        estimate std.error statistic  p.value\n  &lt;chr&gt;          &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;    &lt;dbl&gt;\n1 (Intercept)  61.1      0.703       86.9  7.06e-12\n2 temp         -0.0177   0.00115    -15.4  1.18e- 6\n3 thick_X2      0.804    0.150        5.38 1.03e- 3\n\n\n\n\n\n2.2.3 Ex 2.16: Interaction\nYou can revise the workflow by updating recipe to include interaction term.\n\n2.2.3.1 Update recipe\nAdd step_interact() to existing recipe to add interaction term. Because a dummy variable will be used in this step, use starts_with(\"thick\") to capture dummy variable names, instead of using original variable name.\n\nrec_interaction &lt;-\n  rec |&gt; \n  step_interact(terms = ~ temp:starts_with(\"thick\"))\n\n\nrec_interaction\n\n\n\n\n── Recipe ──────────────────────────────────────────────────────────────────────\n\n\n\n\n\n── Inputs \n\n\nNumber of variables by role\n\n\noutcome:   1\npredictor: 2\n\n\n\n\n\n── Operations \n\n\n• Variable mutation for: factor(thick, levels = c(6, 2))\n\n\n• Dummy variables from: thick\n\n\n• Interactions with: temp:starts_with(\"thick\")\n\n\nAgain, quickly check a resulting training data.\n\nrec_interaction |&gt; \n  prep() |&gt; \n  juice()\n\n# A tibble: 10 × 4\n    temp     y thick_X2 temp_x_thick_X2\n   &lt;dbl&gt; &lt;dbl&gt;    &lt;dbl&gt;           &lt;dbl&gt;\n 1   540  52.5        1             540\n 2   660  50.2        1             660\n 3   610  51.3        1             610\n 4   710  49.1        1             710\n 5   570  50.8        0               0\n 6   700  48.7        0               0\n 7   560  51.2        0               0\n 8   600  50.8        0               0\n 9   680  49.3        0               0\n10   530  51.5        0               0\n\n\n\n\n2.2.3.2 Update workflow\nUpdate workflow by calling update_recipe().\n\nwflow_interaction &lt;- \n  wflow |&gt; \n  update_recipe(rec_interaction)\n\n\nwflow_interaction\n\n══ Workflow ════════════════════════════════════════════════════════════════════\nPreprocessor: Recipe\nModel: linear_reg()\n\n── Preprocessor ────────────────────────────────────────────────────────────────\n3 Recipe Steps\n\n• step_mutate()\n• step_dummy()\n• step_interact()\n\n── Model ───────────────────────────────────────────────────────────────────────\nLinear Regression Model Specification (regression)\n\nComputational engine: lm \n\n\n\n\n2.2.3.3 Estimate a model with the new workflow\nCall fit() function with the new workflow.\n\nlm_model_interaction &lt;- \n  wflow_interaction |&gt; \n  fit(dat1)\n\nSee estimated coefficients.\n\ntidy(lm_model_interaction)\n\n# A tibble: 4 × 5\n  term            estimate std.error statistic  p.value\n  &lt;chr&gt;              &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;    &lt;dbl&gt;\n1 (Intercept)     60.1       0.750       80.2  2.53e-10\n2 temp            -0.0161    0.00123    -13.1  1.23e- 5\n3 thick_X2         3.28      1.21         2.71 3.52e- 2\n4 temp_x_thick_X2 -0.00399   0.00194     -2.05 8.57e- 2",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Regression</span>"
    ]
  },
  {
    "objectID": "ch03_regularized_regression.html",
    "href": "ch03_regularized_regression.html",
    "title": "3  Regularized regression",
    "section": "",
    "text": "3.1 Examples 3.1 - 3.2",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Regularized regression</span>"
    ]
  },
  {
    "objectID": "ch03_regularized_regression.html#examples-3.1---3.2",
    "href": "ch03_regularized_regression.html#examples-3.1---3.2",
    "title": "3  Regularized regression",
    "section": "",
    "text": "3.1.1 Load data\n\ndat1 &lt;- read_csv(\"data/ch3_dat1.csv\")\n\nRows: 7 Columns: 3\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \",\"\ndbl (3): x1, x2, y\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n\n\nNumber of objects\n\nN &lt;- nrow(dat1)\nN\n\n[1] 7\n\n\n\n\n3.1.2 Standardize input variables\nDefine a recipe to standardize each input variable\n\nrec &lt;- \n  recipe(y ~ x1 + x2, data = dat1) |&gt; \n  step_normalize(x1, x2)\n\n\n\n3.1.3 Ex 3.1: Lasso\n\n3.1.3.1 Basics\nRegularized regression model is still defined by linear_reg(), but with additional arguments penalty and mixture, where penalty is for the amount of regularization, while mixture is for proportion of L1 regularization. Set mixture = 1 to be Lasso. And let us set penalty = 0 to check a regression coefficient without regularization.\nDefault engine \"lm\" does not support regularized regression, so you should set a specific engine that support regularized regression. Let us use \"glmnet\" here.\n\nlasso_model &lt;- \n  linear_reg(penalty = 0, mixture = 1) |&gt; \n  set_engine(\"glmnet\")\n\nThis is translated into {glmnet} syntax like below:\n\nlasso_model |&gt; \n  translate()\n\nLinear Regression Model Specification (regression)\n\nMain Arguments:\n  penalty = 0\n  mixture = 1\n\nComputational engine: glmnet \n\nModel fit template:\nglmnet::glmnet(x = missing_arg(), y = missing_arg(), weights = missing_arg(), \n    alpha = 1, family = \"gaussian\")\n\n\nDefine a workflow for Lasso regression.\n\nlasso_wflow &lt;- \n  workflow() |&gt; \n  add_recipe(rec) |&gt; \n  add_model(lasso_model)\n\nThen estimate the model.\n\nmodel_fit &lt;- \n  lasso_wflow |&gt; \n  fit(data = dat1)\n\n\ntidy(model_fit)\n\n\nAttaching package: 'Matrix'\n\n\nThe following objects are masked from 'package:tidyr':\n\n    expand, pack, unpack\n\n\nLoaded glmnet 4.1-8\n\n\n# A tibble: 3 × 3\n  term        estimate penalty\n  &lt;chr&gt;          &lt;dbl&gt;   &lt;dbl&gt;\n1 (Intercept) 6.28e-17       0\n2 x1          2.01e+ 0       0\n3 x2          1.26e- 1       0\n\n\n\n\n3.1.3.2 Set regularization path\nPrevious result is slightly different from a result of typical linear regression.\n\nworkflow() |&gt; \n  add_recipe(rec) |&gt; \n  add_model(linear_reg()) |&gt; \n  fit(data = dat1) |&gt; \n  tidy()\n\n# A tibble: 3 × 5\n  term        estimate std.error statistic p.value\n  &lt;chr&gt;          &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;   &lt;dbl&gt;\n1 (Intercept) 3.37e-18     0.172  1.96e-17 1      \n2 x1          2.02e+ 0     0.306  6.60e+ 0 0.00272\n3 x2          1.34e- 1     0.306  4.38e- 1 0.684  \n\n\nA region is that {glmnet} consumes additional parameter lambda to set a series of values called “regularization path”, and the model approximates between the closet path values. So, if you want to obtain more correct coefficient or prediction associated with a particular penalty amount, include the value to regularization path with glmnet-specific optional parameter path_values in set_engine(). See Technical aspects of the glmnet model\n\nregularization_path &lt;- c(3:0) / (2 * N - 1)\n\nlasso_model &lt;- \n  linear_reg(penalty = 0, mixture = 1) |&gt; \n  set_engine(\"glmnet\", path_values = regularization_path)\n\n\nlasso_model |&gt; \n  translate()\n\nLinear Regression Model Specification (regression)\n\nMain Arguments:\n  penalty = 0\n  mixture = 1\n\nComputational engine: glmnet \n\nModel fit template:\nglmnet::glmnet(x = missing_arg(), y = missing_arg(), weights = missing_arg(), \n    lambda = regularization_path, alpha = 1, family = \"gaussian\")\n\n\nDefine a workflow for Lasso regression.\n\nlasso_wflow &lt;- \n  workflow() |&gt; \n  add_recipe(rec) |&gt; \n  add_model(lasso_model)\n\nmodel_fit &lt;- \n  lasso_wflow |&gt; \n  fit(data = dat1)\n\n\ntidy(model_fit)\n\n# A tibble: 3 × 3\n  term        estimate penalty\n  &lt;chr&gt;          &lt;dbl&gt;   &lt;dbl&gt;\n1 (Intercept) 6.35e-17       0\n2 x1          2.02e+ 0       0\n3 x2          1.33e- 1       0\n\n\nTo obtain coefficients for all four penalty amounts of interest in the regularization path, we can extract the glmnet object and apply specific function.\n\nmodel_fit |&gt; \n  extract_fit_parsnip() |&gt; \n  extract_fit_engine() |&gt; \n  predict(type = \"coefficients\") |&gt; \n  round(4)\n\n3 x 4 sparse Matrix of class \"dgCMatrix\"\n                s0     s1     s2     s3\n(Intercept) 0.0000 0.0000 0.0000 0.0000\nx1          1.8771 1.9279 1.9743 2.0207\nx2          .      0.0409 0.0872 0.1335\n\n\n\n\n\n3.1.4 Ex 3.2: Ridge\nRidge is very similar to Lasso, except setting mixture = 0. Let us define the workflow for ridge regression.\n\nregularization_path &lt;- c(3:0) / (N - 1) * 2\n\nridge_model &lt;- \n  linear_reg(penalty = 0, mixture = 0) |&gt; \n  set_engine(\"glmnet\", path_values = regularization_path)\n\nridge_wflow &lt;- \n  workflow() |&gt; \n  add_recipe(rec) |&gt; \n  add_model(ridge_model)\n\n\nmodel_fit &lt;- \n  ridge_wflow |&gt; \n  fit(data = dat1)\n\nmodel_fit |&gt; \n  extract_fit_parsnip() |&gt; \n  extract_fit_engine() |&gt; \n  predict(type = \"coefficients\") |&gt; \n  round(4)\n\n3 x 4 sparse Matrix of class \"dgCMatrix\"\n                s0     s1     s2     s3\n(Intercept) 0.0000 0.0000 0.0000 0.0000\nx1          1.1178 1.2687 1.5069 2.0194\nx2          0.5667 0.5477 0.4639 0.1345\n\n\nEven though obtaining coefficients is somewhat tedious because it requires extraction of underlying model objects, making a prediction from all the penalty amounts of interest is a little bit more convenient by using multi_predict() function. However, data preprocessing for new data needs to be separately done.\n\nridge_pred &lt;- \n  model_fit |&gt; \n  extract_fit_parsnip() |&gt; \n  multi_predict(\n    new_data = bake(prep(rec, dat1), dat1)[c('x1', 'x2')], \n    penalty = regularization_path\n  )\n\nLet us see prediction results.\n\ndat1 |&gt; \n  bind_cols(ridge_pred) |&gt; \n  unnest(.pred)\n\n# A tibble: 28 × 5\n      x1    x2     y penalty .pred\n   &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;   &lt;dbl&gt; &lt;dbl&gt;\n 1    -6    -3    -3   0     -2.77\n 2    -6    -3    -3   0.333 -2.36\n 3    -6    -3    -3   0.667 -2.11\n 4    -6    -3    -3   1     -1.93\n 5    -4    -1    -2   0     -1.81\n 6    -4    -1    -2   0.333 -1.45\n 7    -4    -1    -2   0.667 -1.26\n 8    -4    -1    -2   1     -1.13\n 9    -3    -2    -1   0     -1.40\n10    -3    -2    -1   0.333 -1.24\n# ℹ 18 more rows",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Regularized regression</span>"
    ]
  },
  {
    "objectID": "ch04_dimension_reduction.html",
    "href": "ch04_dimension_reduction.html",
    "title": "4  Dimension reduction",
    "section": "",
    "text": "4.1 Examples 4.10",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Dimension reduction</span>"
    ]
  },
  {
    "objectID": "ch04_dimension_reduction.html#examples-4.10",
    "href": "ch04_dimension_reduction.html#examples-4.10",
    "title": "4  Dimension reduction",
    "section": "",
    "text": "4.1.1 Load data\n\ndat2 &lt;- read_csv(\"data/ch4_dat2.csv\", locale = locale(encoding = \"euc-kr\"))\n\nRows: 18 Columns: 6\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \",\"\nchr (1): ID\ndbl (5): X1, X2, X3, X4, X5\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n\ndat2\n\n# A tibble: 18 × 6\n   ID              X1    X2    X3    X4    X5\n   &lt;chr&gt;        &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;\n 1 SK증권        2.43 11.1   18.5 442.   0.9 \n 2 교보증권      3.09  9.95  29.5 239.   0.9 \n 3 대신증권      2.22  6.86  28.6 249.   0.69\n 4 대우증권      5.76 23.2   23.5 326.   1.43\n 5 동부증권      1.6   5.64  25.6 290.   1.42\n 6 메리츠증권    3.53 10.6   32.2 210.   1.17\n 7 미래에셋증권  4.26 15.6   24.4 310.   0.81\n 8 부국증권      3.86  5.5   70.7  41.4  0.81\n 9 브릿지증권    4.09  6.44  64.4  55.3  0.32\n10 삼성증권      2.73 10.7   24.4 310.   0.64\n11 서울증권      2.03  4.5   42.5 135.   0.59\n12 신영증권      1.96  8.92  18.5 441.   1.07\n13 신흥증권      3.25  7.96  40.4 147.   1.19\n14 우리투자증권  2.01 10.3   17.5 473.   1.25\n15 유화증권      2.28  3.65  63.7  57.0  0.12\n16 한양증권      4.51  7.5   63.5  57.4  0.8 \n17 한화증권      3.29 12.4   24.5 309.   0.57\n18 현대증권      1.73  7.57  19.6 410.   1.19\n\n\n\n\n4.1.2 Principal component analysis\nDefine recipe for principal component analysis. Use all variables except ID, and standardize each variable before conducting PCA.\n\nrec_pca &lt;- \n  recipe(~ ., data = dat2) |&gt; \n  step_rm(ID) |&gt; \n  step_normalize(everything()) |&gt; \n  step_pca(everything())\n\nApplying tidy() on a recipe object returns a data frame that each row represents each step.\n\ntidy(rec_pca)\n\n# A tibble: 3 × 6\n  number operation type      trained skip  id             \n   &lt;int&gt; &lt;chr&gt;     &lt;chr&gt;     &lt;lgl&gt;   &lt;lgl&gt; &lt;chr&gt;          \n1      1 step      rm        FALSE   FALSE rm_rJHMJ       \n2      2 step      normalize FALSE   FALSE normalize_ssAMh\n3      3 step      pca       FALSE   FALSE pca_BHALr      \n\n\nLet us estimate PCA model.\n\npca_estimates &lt;- prep(rec_pca)\n\nApplying tidy() on estimated recipe also returns a data frame that each row represents each step. Please see that values in column trained is TRUE when passing the estimated recipe, i.e. output of prep().\n\ntidy(pca_estimates)\n\n# A tibble: 3 × 6\n  number operation type      trained skip  id             \n   &lt;int&gt; &lt;chr&gt;     &lt;chr&gt;     &lt;lgl&gt;   &lt;lgl&gt; &lt;chr&gt;          \n1      1 step      rm        TRUE    FALSE rm_rJHMJ       \n2      2 step      normalize TRUE    FALSE normalize_ssAMh\n3      3 step      pca       TRUE    FALSE pca_BHALr      \n\n\nIn this example, PCA is the 3rd step. By passing argument number = 3 when calling tidy(), you can extract loadings.\n\ntidy(pca_estimates, number = 3)\n\n# A tibble: 25 × 4\n   terms   value component id       \n   &lt;chr&gt;   &lt;dbl&gt; &lt;chr&gt;     &lt;chr&gt;    \n 1 X1     0.0761 PC1       pca_BHALr\n 2 X2    -0.395  PC1       pca_BHALr\n 3 X3     0.570  PC1       pca_BHALr\n 4 X4    -0.560  PC1       pca_BHALr\n 5 X5    -0.448  PC1       pca_BHALr\n 6 X1     0.780  PC2       pca_BHALr\n 7 X2     0.565  PC2       pca_BHALr\n 8 X3     0.162  PC2       pca_BHALr\n 9 X4    -0.197  PC2       pca_BHALr\n10 X5     0.0864 PC2       pca_BHALr\n# ℹ 15 more rows\n\n\nLet’s see entire loading matrix for this example.\n\ntidy(pca_estimates, number = 3) |&gt; \n  pivot_wider(names_from = component, values_from = value)\n\n# A tibble: 5 × 7\n  terms id            PC1     PC2       PC3      PC4     PC5\n  &lt;chr&gt; &lt;chr&gt;       &lt;dbl&gt;   &lt;dbl&gt;     &lt;dbl&gt;    &lt;dbl&gt;   &lt;dbl&gt;\n1 X1    pca_BHALr  0.0761  0.780   0.000892  0.141    0.605 \n2 X2    pca_BHALr -0.395   0.565  -0.295    -0.118   -0.651 \n3 X3    pca_BHALr  0.570   0.162   0.241     0.638   -0.429 \n4 X4    pca_BHALr -0.560  -0.197  -0.257     0.748    0.150 \n5 X5    pca_BHALr -0.448   0.0864  0.888     0.00367 -0.0571\n\n\nBy additionally passing type = \"variance\" argument, you can see eigenvalues from the rows associated with terms == \"variance\", and derived statistics.\n\ntidy(pca_estimates, number = 3, type = \"variance\")\n\n# A tibble: 20 × 4\n   terms                          value component id       \n   &lt;chr&gt;                          &lt;dbl&gt;     &lt;int&gt; &lt;chr&gt;    \n 1 variance                      2.76           1 pca_BHALr\n 2 variance                      1.61           2 pca_BHALr\n 3 variance                      0.551          3 pca_BHALr\n 4 variance                      0.0641         4 pca_BHALr\n 5 variance                      0.0183         5 pca_BHALr\n 6 cumulative variance           2.76           1 pca_BHALr\n 7 cumulative variance           4.37           2 pca_BHALr\n 8 cumulative variance           4.92           3 pca_BHALr\n 9 cumulative variance           4.98           4 pca_BHALr\n10 cumulative variance           5              5 pca_BHALr\n11 percent variance             55.2            1 pca_BHALr\n12 percent variance             32.1            2 pca_BHALr\n13 percent variance             11.0            3 pca_BHALr\n14 percent variance              1.28           4 pca_BHALr\n15 percent variance              0.365          5 pca_BHALr\n16 cumulative percent variance  55.2            1 pca_BHALr\n17 cumulative percent variance  87.3            2 pca_BHALr\n18 cumulative percent variance  98.4            3 pca_BHALr\n19 cumulative percent variance  99.6            4 pca_BHALr\n20 cumulative percent variance 100              5 pca_BHALr",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Dimension reduction</span>"
    ]
  },
  {
    "objectID": "ch04_dimension_reduction.html#examples-4.12",
    "href": "ch04_dimension_reduction.html#examples-4.12",
    "title": "4  Dimension reduction",
    "section": "4.2 Examples 4.12",
    "text": "4.2 Examples 4.12\n\n4.2.1 Load data\n\ndat3 &lt;- read_csv(\"data/ch4_dat3.csv\")\n\nRows: 6 Columns: 4\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \",\"\ndbl (4): x1, x2, x3, y\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n\ndat3\n\n# A tibble: 6 × 4\n     x1    x2    x3     y\n  &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;\n1    -3    -3     5   -30\n2    -2    -3     7   -20\n3     0     0     4     0\n4     1     2     0     5\n5     2     2    -5    10\n6     2     2   -11    35\n\n\n\n\n4.2.2 Principal component regression (PCR)\nTo use principal component scores as input variables of a regression model, it is recommended to use workflow.\nFirst, let us define a recipe of data preprocessing including PCA. Let us create only two principal components, by passing argument num_comp = 2.\n\nrec_pca &lt;- \n  recipe(y ~ ., data = dat3) |&gt; \n  step_center(all_predictors()) |&gt; \n  step_pca(all_predictors(), num_comp = 2)\n\nLet us define a model. We will use just a default linear regression.\n\nlm_model &lt;- linear_reg()\n\nThen let us define a workflow.\n\npcr_wflow &lt;-\n  workflow() |&gt; \n  add_recipe(rec_pca) |&gt; \n  add_model(lm_model)\n\nFinally, we will train the workflow on training data.\n\npcr_fit &lt;- fit(pcr_wflow, data = dat3)\n\nSee the estimated coefficients on principal components.\n\ntidy(pcr_fit)\n\n# A tibble: 3 × 5\n  term         estimate std.error statistic p.value\n  &lt;chr&gt;           &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;   &lt;dbl&gt;\n1 (Intercept) -5.02e-15     3.58  -1.40e-15  1.00  \n2 PC1          2.92e+ 0     0.530  5.51e+ 0  0.0118\n3 PC2         -2.54e+ 0     2.35  -1.08e+ 0  0.360 \n\n\n\n\n4.2.3 Prediction\nLet us predict response variable value by using the estimated PCR model by calling predict() function.\n\ndat3 |&gt; \n  bind_cols(predict(pcr_fit, new_data = dat3))\n\n# A tibble: 6 × 5\n     x1    x2    x3     y  .pred\n  &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;  &lt;dbl&gt;\n1    -3    -3     5   -30 -23.2 \n2    -2    -3     7   -20 -24.6 \n3     0     0     4     0  -6.95\n4     1     2     0     5   7.57\n5     2     2    -5    10  18.4 \n6     2     2   -11    35  28.8",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Dimension reduction</span>"
    ]
  },
  {
    "objectID": "ch04_dimension_reduction.html#examples-4.14---4.15",
    "href": "ch04_dimension_reduction.html#examples-4.14---4.15",
    "title": "4  Dimension reduction",
    "section": "4.3 Examples 4.14 - 4.15",
    "text": "4.3 Examples 4.14 - 4.15\n\n4.3.1 Load data\n\ndat3 &lt;- read_csv(\"data/ch4_dat3.csv\")\n\nRows: 6 Columns: 4\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \",\"\ndbl (4): x1, x2, x3, y\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n\ndat3\n\n# A tibble: 6 × 4\n     x1    x2    x3     y\n  &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;\n1    -3    -3     5   -30\n2    -2    -3     7   -20\n3     0     0     4     0\n4     1     2     0     5\n5     2     2    -5    10\n6     2     2   -11    35\n\n\n\n\n4.3.2 Partial least squares regression\nEstimating partial least squares(PLS) model within tidymodels framework requires some additional settings. First, you need to install {mixOmics} package to use it as an engine. {mixOmics} exists not on CRAN but on Bioconductor repository, so you should follow the installation guidance. Then, using parsnip::pls() requires installing and loading an extension package {plsmod}. Please see an example.\nNow, let us define a PLS model. Pass scale = FALSE in set_engine() when you want to use original scale for predictor variables instead of standardizing them.\n\nlibrary(plsmod)\n\npls_model &lt;- \n  pls(num_comp = 2) |&gt; \n  set_engine(\"mixOmics\", scale = FALSE) |&gt; \n  set_mode(\"regression\")\n\nLet’s estimate the PLS model.\n\npls_fit &lt;- \n  pls_model |&gt; \n  fit(y ~ ., data = dat3)\n\n\n\n4.3.3 Estimated model\nExtracting each estimated matrix from PLS model mostly requires direct access to model engine object by extract_fit_engine(); not much convinient interface through tidymodels framework.\nSee latent matrix (T):\n\npls_fit |&gt; \n  extract_fit_engine() |&gt; \n  pluck(\"variates\", \"X\")\n\n       comp1      comp2\n1 -6.3186736 -2.0166999\n2 -7.8514212 -0.5903390\n3 -3.6285452  1.5267780\n4  0.9071363  1.9538097\n5  5.7243429  0.7083091\n6 11.1671608 -1.5818579\n\n\nX-loading matrix (P):\n\npls_fit |&gt; \n  extract_fit_engine() |&gt; \n  pluck(\"mat.c\")\n\n         [,1]      [,2]\nx1  0.2539947 0.5481649\nx2  0.2860734 0.7356762\nx3 -0.9248982 0.4238016\n\n\nLoading weight matrix (W):\n\npls_fit |&gt; \n  extract_fit_engine() |&gt; \n  pluck(\"loadings\", \"X\")\n\n        comp1     comp2\nx1  0.2815251 0.6510676\nx2  0.3128056 0.6321920\nx3 -0.9071363 0.4200527\n\n\nor, simply\n\ntidy(pls_fit)\n\n# A tibble: 8 × 4\n  term   value type       component\n  &lt;chr&gt;  &lt;dbl&gt; &lt;chr&gt;          &lt;dbl&gt;\n1 x1     0.282 predictors         1\n2 x1     0.651 predictors         2\n3 x2     0.313 predictors         1\n4 x2     0.632 predictors         2\n5 x3    -0.907 predictors         1\n6 x3     0.420 predictors         2\n7 Y      1     outcomes           1\n8 Y      1     outcomes           2\n\n\nPlease note that loadings for y are different from the book example.\nWeight matrix related to original predictors are\n\npls_fit |&gt; \n  extract_fit_engine() |&gt; \n  pluck(\"loadings.star\", 1)\n\n         [,1]      [,2]\nx1  0.2815251 0.6629719\nx2  0.3128056 0.6454189\nx3 -0.9071363 0.3816945\n\n\n\n\n4.3.4 Prediction\nPredicting expected response variable value is easy within tidymodels framework, by calling predict() function.\n\ndat3 |&gt; \n  bind_cols(predict(pls_fit, new_data = dat3))\n\n# A tibble: 6 × 5\n     x1    x2    x3     y  .pred\n  &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;  &lt;dbl&gt;\n1    -3    -3     5   -30 -23.5 \n2    -2    -3     7   -20 -24.5 \n3     0     0     4     0  -6.82\n4     1     2     0     5   7.52\n5     2     2    -5    10  18.5 \n6     2     2   -11    35  28.7",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Dimension reduction</span>"
    ]
  }
]