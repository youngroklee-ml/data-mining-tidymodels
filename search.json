[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Data mining with {tidymodels}",
    "section": "",
    "text": "1 Preface\nThis is a book to convert example code on https://youngroklee-ml.github.io/data-mining-techniques/ into the {tidymodels} framework.",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Preface</span>"
    ]
  },
  {
    "objectID": "index.html#tidymodels",
    "href": "index.html#tidymodels",
    "title": "Data mining with {tidymodels}",
    "section": "1.1 Tidymodels",
    "text": "1.1 Tidymodels\nThe tidymodels framework is a collection of packages to provide intuitive and unified interface for modeling and machine learning. To get more information, please see the following materials:\n\ntidymodels website: https://www.tidymodels.org\n“Tidy modeling with R” by Max Kuhn and Julia Silge: https://www.tmwr.org",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Preface</span>"
    ]
  },
  {
    "objectID": "index.html#model-engine-packages",
    "href": "index.html#model-engine-packages",
    "title": "Data mining with {tidymodels}",
    "section": "1.2 Model engine packages",
    "text": "1.2 Model engine packages\nEach modeling packages need to be separately installed to use it within the tidymodels framework. Example codes in this book require the following R packages.\n\nlibrary(glmnet)\nlibrary(mixOmics)\nlibrary(kknn)\nlibrary(klaR)\nlibrary(naivebayes)",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Preface</span>"
    ]
  },
  {
    "objectID": "ch02_regression.html",
    "href": "ch02_regression.html",
    "title": "2  Regression",
    "section": "",
    "text": "2.1 Examples 2.3 - 2.5, 2.7, 2.10 - 2.11",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Regression</span>"
    ]
  },
  {
    "objectID": "ch02_regression.html#examples-2.3---2.5-2.7-2.10---2.11",
    "href": "ch02_regression.html#examples-2.3---2.5-2.7-2.10---2.11",
    "title": "2  Regression",
    "section": "",
    "text": "2.1.1 Load data\n\ndat1 &lt;- read_csv(\"data/ch2_reg1.csv\")\n\nRows: 10 Columns: 4\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \",\"\ndbl (4): ID, age, height, weight\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n\nprint(dat1)\n\n# A tibble: 10 × 4\n      ID   age height weight\n   &lt;dbl&gt; &lt;dbl&gt;  &lt;dbl&gt;  &lt;dbl&gt;\n 1     1    21    170     60\n 2     2    47    167     65\n 3     3    36    173     67\n 4     4    15    165     54\n 5     5    54    168     73\n 6     6    25    177     71\n 7     7    32    169     68\n 8     8    18    172     62\n 9     9    43    171     66\n10    10    28    175     68\n\n\n\n\n2.1.2 Ex 2.3: Estimate coefficients\nUse {parsnip} package, a part of {tidymodels}, that provide a unified modeling interface.\nDefine a model type and engine.\n\nmodel &lt;- \n  linear_reg() |&gt;\n  set_engine(\"lm\") # \"lm\" is a default engine for `linear_reg()`\n\nEstimate the model by calling fit() function with formula and training data.\n\nmodel_fit &lt;- \n  model |&gt; \n  fit(weight ~ age + height, data = dat1)\n\nLet’s print estimation results.\n\nprint(model_fit)\n\nparsnip model object\n\n\nCall:\nstats::lm(formula = weight ~ age + height, data = data)\n\nCoefficients:\n(Intercept)          age       height  \n  -108.1672       0.3291       0.9553  \n\n\nbroom::tidy() can extract coefficient statistics as data frame, including estimates and test statistics. Column estimate represents the coefficient estimates.\n\ntidy(model_fit)\n\n# A tibble: 3 × 5\n  term        estimate std.error statistic p.value\n  &lt;chr&gt;          &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;   &lt;dbl&gt;\n1 (Intercept) -108.      42.1        -2.57 0.0371 \n2 age            0.329    0.0692      4.75 0.00208\n3 height         0.955    0.244       3.91 0.00579\n\n\n\n\n2.1.3 Ex 2.4: Estimate variance of error term\nbroom::glance() provides model-level statistics.\n\nglance(model_fit)\n\n# A tibble: 1 × 12\n  r.squared adj.r.squared sigma statistic p.value    df logLik   AIC   BIC\n      &lt;dbl&gt;         &lt;dbl&gt; &lt;dbl&gt;     &lt;dbl&gt;   &lt;dbl&gt; &lt;dbl&gt;  &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;\n1     0.822         0.771  2.65      16.1 0.00239     2  -22.2  52.3  53.5\n# ℹ 3 more variables: deviance &lt;dbl&gt;, df.residual &lt;int&gt;, nobs &lt;int&gt;\n\n\nsigma is the estimate of standard deviation of the error term, so just square it to estimate the variance of the error term.\n\nglance(model_fit)[[\"sigma\"]]^2\n\n[1] 7.038464\n\n\n\n\n2.1.4 Ex 2.5: Test a model\nCall extract_fit_engine() when you need to explicitly use underlying lm object.\n\nextract_fit_engine(model_fit)\n\n\nCall:\nstats::lm(formula = weight ~ age + height, data = data)\n\nCoefficients:\n(Intercept)          age       height  \n  -108.1672       0.3291       0.9553  \n\n\nThis is important when a function that you call require the underlying engine’s object, not tidymodels framework’s wrapper class. anova() function to conduct ANOVA test is one of such functions that you need to pass lm object.\n\nmodel_fit |&gt; \n  extract_fit_engine() |&gt; \n  anova()\n\nAnalysis of Variance Table\n\nResponse: weight\n          Df  Sum Sq Mean Sq F value   Pr(&gt;F)   \nage        1 119.299 119.299   16.95 0.004476 **\nheight     1 107.831 107.831   15.32 0.005793 **\nResiduals  7  49.269   7.038                    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\n\n\n2.1.5 Ex 2.7: Test each coefficient\nAs seen above, tidy() function returns data frame that contain test statistics.\n\ntidy(model_fit)\n\n# A tibble: 3 × 5\n  term        estimate std.error statistic p.value\n  &lt;chr&gt;          &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;   &lt;dbl&gt;\n1 (Intercept) -108.      42.1        -2.57 0.0371 \n2 age            0.329    0.0692      4.75 0.00208\n3 height         0.955    0.244       3.91 0.00579\n\n\n\n\n2.1.6 Ex 2.10: Mean prediciton\nLet’s create a new data set as a tidyverse data frame tibble.\n\nnewdata &lt;- tibble(age = 40, height = 170)\n\nMake a mean prediction by calling predict() function, and append it to the data set as a column by calling bind_cols().\n\nnewdata |&gt; \n  bind_cols(predict(model_fit, new_data = newdata))\n\n# A tibble: 1 × 3\n    age height .pred\n  &lt;dbl&gt;  &lt;dbl&gt; &lt;dbl&gt;\n1    40    170  67.4\n\n\nAlso, add 95% confidence interval by calling predict() function with argument type = \"conf_int\".\n\nnewdata |&gt; \n  bind_cols(predict(model_fit, new_data = newdata)) |&gt; \n  bind_cols(predict(model_fit, new_data = newdata, type = \"conf_int\"))\n\n# A tibble: 1 × 5\n    age height .pred .pred_lower .pred_upper\n  &lt;dbl&gt;  &lt;dbl&gt; &lt;dbl&gt;       &lt;dbl&gt;       &lt;dbl&gt;\n1    40    170  67.4        65.0        69.8\n\n\n\n\n2.1.7 Ex 2.11 Prediction interval\nInstead of confidence interval, you can add prediction interval by passing argument type = \"pred_int\".\n\nnewdata |&gt; \n  bind_cols(predict(model_fit, new_data = newdata)) |&gt; \n  bind_cols(predict(model_fit, new_data = newdata, type = \"pred_int\"))\n\n# A tibble: 1 × 5\n    age height .pred .pred_lower .pred_upper\n  &lt;dbl&gt;  &lt;dbl&gt; &lt;dbl&gt;       &lt;dbl&gt;       &lt;dbl&gt;\n1    40    170  67.4        60.7        74.1",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Regression</span>"
    ]
  },
  {
    "objectID": "ch02_regression.html#examples-2.14-2.16",
    "href": "ch02_regression.html#examples-2.14-2.16",
    "title": "2  Regression",
    "section": "2.2 Examples 2.14, 2.16",
    "text": "2.2 Examples 2.14, 2.16\n\n2.2.1 Load data\n\ndat1 &lt;- read_csv(\"data/ch2_coil.csv\")\n\nRows: 10 Columns: 3\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \",\"\ndbl (3): temp, thick, y\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n\n\n\n\n2.2.2 Ex 2.14: Indicator variable\n\n2.2.2.1 Set indicator variable\nDefine feature engineering steps with {recipes} package, a part of {tidymodels}.\nFirst, call recipe() to define input variables, output variable, and data.\n\nrec &lt;- \n  recipe(y ~ temp + thick, data = dat1)\n\nSee the recipe recognizes that there are one outcome variable and two predictor variables.\n\nrec\n\n\n\n\n── Recipe ──────────────────────────────────────────────────────────────────────\n\n\n\n\n\n── Inputs \n\n\nNumber of variables by role\n\n\noutcome:   1\npredictor: 2\n\n\nYou want to consider thick as a categorical variable by using factor() inside step_mutate() and create a dummy variable by adding step_dummy().\n\nrec &lt;- \n  rec |&gt; \n  step_mutate(thick = factor(thick, levels = c(6, 2))) |&gt; \n  step_dummy(thick)\n\nNow check that the updated recipe recognize thick as a dummy variable.\n\nrec\n\n\n\n\n── Recipe ──────────────────────────────────────────────────────────────────────\n\n\n\n\n\n── Inputs \n\n\nNumber of variables by role\n\n\noutcome:   1\npredictor: 2\n\n\n\n\n\n── Operations \n\n\n• Variable mutation for: factor(thick, levels = c(6, 2))\n\n\n• Dummy variables from: thick\n\n\nOf course, you can make write it as one pipeline\n\nrec &lt;- \n  recipe(y ~ temp + thick, data = dat1) |&gt; \n  step_mutate(thick = factor(thick, levels = c(6, 2))) |&gt; \n  step_dummy(thick)\n\nQuickly see what the output would look like. Here, prep() is a function to train feature engineering, and juice() is a result of applying the feature engineering to training data.\n\nrec |&gt; \n  prep() |&gt; \n  juice()\n\n# A tibble: 10 × 3\n    temp     y thick_X2\n   &lt;dbl&gt; &lt;dbl&gt;    &lt;dbl&gt;\n 1   540  52.5        1\n 2   660  50.2        1\n 3   610  51.3        1\n 4   710  49.1        1\n 5   570  50.8        0\n 6   700  48.7        0\n 7   560  51.2        0\n 8   600  50.8        0\n 9   680  49.3        0\n10   530  51.5        0\n\n\n\n\n2.2.2.2 Include feature engineering into a workflow\nLet’s start with defining model type and engine as we did in the first example above.\n\nmodel &lt;- \n  linear_reg() |&gt; \n  set_engine(\"lm\")\n\nNow, let’s define a “workflow” from {workflows} package, a part of {tidymodels}, to combine a recipe and a model.\n\nwflow &lt;- \n  workflow() |&gt; \n  add_recipe(rec) |&gt; \n  add_model(model) \n\nThe workflow recognizes that rec is a preprocessor before estimating a regression model.\n\nwflow\n\n══ Workflow ════════════════════════════════════════════════════════════════════\nPreprocessor: Recipe\nModel: linear_reg()\n\n── Preprocessor ────────────────────────────────────────────────────────────────\n2 Recipe Steps\n\n• step_mutate()\n• step_dummy()\n\n── Model ───────────────────────────────────────────────────────────────────────\nLinear Regression Model Specification (regression)\n\nComputational engine: lm \n\n\nNow, call fit() function by passing training data to estimate a regression model.\n\nlm_model &lt;- \n  wflow |&gt; \n  fit(dat1)\n\nSee the model estimation results.\n\nlm_model\n\n══ Workflow [trained] ══════════════════════════════════════════════════════════\nPreprocessor: Recipe\nModel: linear_reg()\n\n── Preprocessor ────────────────────────────────────────────────────────────────\n2 Recipe Steps\n\n• step_mutate()\n• step_dummy()\n\n── Model ───────────────────────────────────────────────────────────────────────\n\nCall:\nstats::lm(formula = ..y ~ ., data = data)\n\nCoefficients:\n(Intercept)         temp     thick_X2  \n   61.10797     -0.01768      0.80415  \n\n\nYou can still use broom::tidy() function to return coefficient-level statistics.\n\ntidy(lm_model)\n\n# A tibble: 3 × 5\n  term        estimate std.error statistic  p.value\n  &lt;chr&gt;          &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;    &lt;dbl&gt;\n1 (Intercept)  61.1      0.703       86.9  7.06e-12\n2 temp         -0.0177   0.00115    -15.4  1.18e- 6\n3 thick_X2      0.804    0.150        5.38 1.03e- 3\n\n\n\n\n\n2.2.3 Ex 2.16: Interaction\nYou can revise the workflow by updating recipe to include interaction term.\n\n2.2.3.1 Update recipe\nAdd step_interact() to existing recipe to add interaction term. Because a dummy variable will be used in this step, use starts_with(\"thick\") to capture dummy variable names, instead of using original variable name.\n\nrec_interaction &lt;-\n  rec |&gt; \n  step_interact(terms = ~ temp:starts_with(\"thick\"))\n\n\nrec_interaction\n\n\n\n\n── Recipe ──────────────────────────────────────────────────────────────────────\n\n\n\n\n\n── Inputs \n\n\nNumber of variables by role\n\n\noutcome:   1\npredictor: 2\n\n\n\n\n\n── Operations \n\n\n• Variable mutation for: factor(thick, levels = c(6, 2))\n\n\n• Dummy variables from: thick\n\n\n• Interactions with: temp:starts_with(\"thick\")\n\n\nAgain, quickly check a resulting training data.\n\nrec_interaction |&gt; \n  prep() |&gt; \n  juice()\n\n# A tibble: 10 × 4\n    temp     y thick_X2 temp_x_thick_X2\n   &lt;dbl&gt; &lt;dbl&gt;    &lt;dbl&gt;           &lt;dbl&gt;\n 1   540  52.5        1             540\n 2   660  50.2        1             660\n 3   610  51.3        1             610\n 4   710  49.1        1             710\n 5   570  50.8        0               0\n 6   700  48.7        0               0\n 7   560  51.2        0               0\n 8   600  50.8        0               0\n 9   680  49.3        0               0\n10   530  51.5        0               0\n\n\n\n\n2.2.3.2 Update workflow\nUpdate workflow by calling update_recipe().\n\nwflow_interaction &lt;- \n  wflow |&gt; \n  update_recipe(rec_interaction)\n\n\nwflow_interaction\n\n══ Workflow ════════════════════════════════════════════════════════════════════\nPreprocessor: Recipe\nModel: linear_reg()\n\n── Preprocessor ────────────────────────────────────────────────────────────────\n3 Recipe Steps\n\n• step_mutate()\n• step_dummy()\n• step_interact()\n\n── Model ───────────────────────────────────────────────────────────────────────\nLinear Regression Model Specification (regression)\n\nComputational engine: lm \n\n\n\n\n2.2.3.3 Estimate a model with the new workflow\nCall fit() function with the new workflow.\n\nlm_model_interaction &lt;- \n  wflow_interaction |&gt; \n  fit(dat1)\n\nSee estimated coefficients.\n\ntidy(lm_model_interaction)\n\n# A tibble: 4 × 5\n  term            estimate std.error statistic  p.value\n  &lt;chr&gt;              &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;    &lt;dbl&gt;\n1 (Intercept)     60.1       0.750       80.2  2.53e-10\n2 temp            -0.0161    0.00123    -13.1  1.23e- 5\n3 thick_X2         3.28      1.21         2.71 3.52e- 2\n4 temp_x_thick_X2 -0.00399   0.00194     -2.05 8.57e- 2",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Regression</span>"
    ]
  },
  {
    "objectID": "ch03_regularized_regression.html",
    "href": "ch03_regularized_regression.html",
    "title": "3  Regularized regression",
    "section": "",
    "text": "3.1 Examples 3.1 - 3.2",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Regularized regression</span>"
    ]
  },
  {
    "objectID": "ch03_regularized_regression.html#examples-3.1---3.2",
    "href": "ch03_regularized_regression.html#examples-3.1---3.2",
    "title": "3  Regularized regression",
    "section": "",
    "text": "3.1.1 Load data\n\ndat1 &lt;- read_csv(\"data/ch3_dat1.csv\")\n\nRows: 7 Columns: 3\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \",\"\ndbl (3): x1, x2, y\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n\n\nNumber of objects\n\nN &lt;- nrow(dat1)\nN\n\n[1] 7\n\n\n\n\n3.1.2 Standardize input variables\nDefine a recipe to standardize each input variable\n\nrec &lt;- \n  recipe(y ~ x1 + x2, data = dat1) |&gt; \n  step_normalize(x1, x2)\n\n\n\n3.1.3 Ex 3.1: Lasso\n\n3.1.3.1 Basics\nRegularized regression model is still defined by linear_reg(), but with additional arguments penalty and mixture, where penalty is for the amount of regularization, while mixture is for proportion of L1 regularization. Set mixture = 1 to be Lasso. And let us set penalty = 0 to check a regression coefficient without regularization.\nDefault engine \"lm\" does not support regularized regression, so you should set a specific engine that support regularized regression. Let us use \"glmnet\" here.\n\nlasso_model &lt;- \n  linear_reg(penalty = 0, mixture = 1) |&gt; \n  set_engine(\"glmnet\")\n\nThis is translated into {glmnet} syntax like below:\n\nlasso_model |&gt; \n  translate()\n\nLinear Regression Model Specification (regression)\n\nMain Arguments:\n  penalty = 0\n  mixture = 1\n\nComputational engine: glmnet \n\nModel fit template:\nglmnet::glmnet(x = missing_arg(), y = missing_arg(), weights = missing_arg(), \n    alpha = 1, family = \"gaussian\")\n\n\nDefine a workflow for Lasso regression.\n\nlasso_wflow &lt;- \n  workflow() |&gt; \n  add_recipe(rec) |&gt; \n  add_model(lasso_model)\n\nThen estimate the model.\n\nmodel_fit &lt;- \n  lasso_wflow |&gt; \n  fit(data = dat1)\n\n\ntidy(model_fit)\n\n\nAttaching package: 'Matrix'\n\n\nThe following objects are masked from 'package:tidyr':\n\n    expand, pack, unpack\n\n\nLoaded glmnet 4.1-8\n\n\n# A tibble: 3 × 3\n  term        estimate penalty\n  &lt;chr&gt;          &lt;dbl&gt;   &lt;dbl&gt;\n1 (Intercept) 6.28e-17       0\n2 x1          2.01e+ 0       0\n3 x2          1.26e- 1       0\n\n\n\n\n3.1.3.2 Set regularization path\nPrevious result is slightly different from a result of typical linear regression.\n\nworkflow() |&gt; \n  add_recipe(rec) |&gt; \n  add_model(linear_reg()) |&gt; \n  fit(data = dat1) |&gt; \n  tidy()\n\n# A tibble: 3 × 5\n  term        estimate std.error statistic p.value\n  &lt;chr&gt;          &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;   &lt;dbl&gt;\n1 (Intercept) 3.37e-18     0.172  1.96e-17 1      \n2 x1          2.02e+ 0     0.306  6.60e+ 0 0.00272\n3 x2          1.34e- 1     0.306  4.38e- 1 0.684  \n\n\nA region is that {glmnet} consumes additional parameter lambda to set a series of values called “regularization path”, and the model approximates between the closet path values. So, if you want to obtain more correct coefficient or prediction associated with a particular penalty amount, include the value to regularization path with glmnet-specific optional parameter path_values in set_engine(). See Technical aspects of the glmnet model\n\nregularization_path &lt;- c(3:0) / (2 * N - 1)\n\nlasso_model &lt;- \n  linear_reg(penalty = 0, mixture = 1) |&gt; \n  set_engine(\"glmnet\", path_values = regularization_path)\n\n\nlasso_model |&gt; \n  translate()\n\nLinear Regression Model Specification (regression)\n\nMain Arguments:\n  penalty = 0\n  mixture = 1\n\nComputational engine: glmnet \n\nModel fit template:\nglmnet::glmnet(x = missing_arg(), y = missing_arg(), weights = missing_arg(), \n    lambda = regularization_path, alpha = 1, family = \"gaussian\")\n\n\nDefine a workflow for Lasso regression.\n\nlasso_wflow &lt;- \n  workflow() |&gt; \n  add_recipe(rec) |&gt; \n  add_model(lasso_model)\n\nmodel_fit &lt;- \n  lasso_wflow |&gt; \n  fit(data = dat1)\n\n\ntidy(model_fit)\n\n# A tibble: 3 × 3\n  term        estimate penalty\n  &lt;chr&gt;          &lt;dbl&gt;   &lt;dbl&gt;\n1 (Intercept) 6.35e-17       0\n2 x1          2.02e+ 0       0\n3 x2          1.33e- 1       0\n\n\nTo obtain coefficients for all four penalty amounts of interest in the regularization path, we can extract the glmnet object and apply specific function.\n\nmodel_fit |&gt; \n  extract_fit_parsnip() |&gt; \n  extract_fit_engine() |&gt; \n  predict(type = \"coefficients\") |&gt; \n  round(4)\n\n3 x 4 sparse Matrix of class \"dgCMatrix\"\n                s0     s1     s2     s3\n(Intercept) 0.0000 0.0000 0.0000 0.0000\nx1          1.8771 1.9279 1.9743 2.0207\nx2          .      0.0409 0.0872 0.1335\n\n\n\n\n\n3.1.4 Ex 3.2: Ridge\nRidge is very similar to Lasso, except setting mixture = 0. Let us define the workflow for ridge regression.\n\nregularization_path &lt;- c(3:0) / (N - 1) * 2\n\nridge_model &lt;- \n  linear_reg(penalty = 0, mixture = 0) |&gt; \n  set_engine(\"glmnet\", path_values = regularization_path)\n\nridge_wflow &lt;- \n  workflow() |&gt; \n  add_recipe(rec) |&gt; \n  add_model(ridge_model)\n\n\nmodel_fit &lt;- \n  ridge_wflow |&gt; \n  fit(data = dat1)\n\nmodel_fit |&gt; \n  extract_fit_parsnip() |&gt; \n  extract_fit_engine() |&gt; \n  predict(type = \"coefficients\") |&gt; \n  round(4)\n\n3 x 4 sparse Matrix of class \"dgCMatrix\"\n                s0     s1     s2     s3\n(Intercept) 0.0000 0.0000 0.0000 0.0000\nx1          1.1178 1.2687 1.5069 2.0194\nx2          0.5667 0.5477 0.4639 0.1345\n\n\nEven though obtaining coefficients is somewhat tedious because it requires extraction of underlying model objects, making a prediction from all the penalty amounts of interest is a little bit more convenient by using multi_predict() function. However, data preprocessing for new data needs to be separately done.\n\nridge_pred &lt;- \n  model_fit |&gt; \n  extract_fit_parsnip() |&gt; \n  multi_predict(\n    new_data = bake(prep(rec, dat1), dat1)[c('x1', 'x2')], \n    penalty = regularization_path\n  )\n\nLet us see prediction results.\n\ndat1 |&gt; \n  bind_cols(ridge_pred) |&gt; \n  unnest(.pred)\n\n# A tibble: 28 × 5\n      x1    x2     y penalty .pred\n   &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;   &lt;dbl&gt; &lt;dbl&gt;\n 1    -6    -3    -3   0     -2.77\n 2    -6    -3    -3   0.333 -2.36\n 3    -6    -3    -3   0.667 -2.11\n 4    -6    -3    -3   1     -1.93\n 5    -4    -1    -2   0     -1.81\n 6    -4    -1    -2   0.333 -1.45\n 7    -4    -1    -2   0.667 -1.26\n 8    -4    -1    -2   1     -1.13\n 9    -3    -2    -1   0     -1.40\n10    -3    -2    -1   0.333 -1.24\n# ℹ 18 more rows",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Regularized regression</span>"
    ]
  },
  {
    "objectID": "ch04_dimension_reduction.html",
    "href": "ch04_dimension_reduction.html",
    "title": "4  Dimension reduction",
    "section": "",
    "text": "4.1 Examples 4.10",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Dimension reduction</span>"
    ]
  },
  {
    "objectID": "ch04_dimension_reduction.html#examples-4.10",
    "href": "ch04_dimension_reduction.html#examples-4.10",
    "title": "4  Dimension reduction",
    "section": "",
    "text": "4.1.1 Load data\n\ndat2 &lt;- read_csv(\"data/ch4_dat2.csv\", locale = locale(encoding = \"euc-kr\"))\n\nRows: 18 Columns: 6\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \",\"\nchr (1): ID\ndbl (5): X1, X2, X3, X4, X5\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n\ndat2\n\n# A tibble: 18 × 6\n   ID              X1    X2    X3    X4    X5\n   &lt;chr&gt;        &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;\n 1 SK증권        2.43 11.1   18.5 442.   0.9 \n 2 교보증권      3.09  9.95  29.5 239.   0.9 \n 3 대신증권      2.22  6.86  28.6 249.   0.69\n 4 대우증권      5.76 23.2   23.5 326.   1.43\n 5 동부증권      1.6   5.64  25.6 290.   1.42\n 6 메리츠증권    3.53 10.6   32.2 210.   1.17\n 7 미래에셋증권  4.26 15.6   24.4 310.   0.81\n 8 부국증권      3.86  5.5   70.7  41.4  0.81\n 9 브릿지증권    4.09  6.44  64.4  55.3  0.32\n10 삼성증권      2.73 10.7   24.4 310.   0.64\n11 서울증권      2.03  4.5   42.5 135.   0.59\n12 신영증권      1.96  8.92  18.5 441.   1.07\n13 신흥증권      3.25  7.96  40.4 147.   1.19\n14 우리투자증권  2.01 10.3   17.5 473.   1.25\n15 유화증권      2.28  3.65  63.7  57.0  0.12\n16 한양증권      4.51  7.5   63.5  57.4  0.8 \n17 한화증권      3.29 12.4   24.5 309.   0.57\n18 현대증권      1.73  7.57  19.6 410.   1.19\n\n\n\n\n4.1.2 Principal component analysis\nDefine recipe for principal component analysis. Use all variables except ID, and standardize each variable before conducting PCA.\n\nrec_pca &lt;- \n  recipe(~ ., data = dat2) |&gt; \n  step_rm(ID) |&gt; \n  step_normalize(everything()) |&gt; \n  step_pca(everything())\n\nApplying tidy() on a recipe object returns a data frame that each row represents each step.\n\ntidy(rec_pca)\n\n# A tibble: 3 × 6\n  number operation type      trained skip  id             \n   &lt;int&gt; &lt;chr&gt;     &lt;chr&gt;     &lt;lgl&gt;   &lt;lgl&gt; &lt;chr&gt;          \n1      1 step      rm        FALSE   FALSE rm_ebdbN       \n2      2 step      normalize FALSE   FALSE normalize_trXdA\n3      3 step      pca       FALSE   FALSE pca_aBb7l      \n\n\nLet us estimate PCA model.\n\npca_estimates &lt;- prep(rec_pca)\n\nApplying tidy() on estimated recipe also returns a data frame that each row represents each step. Please see that values in column trained is TRUE when passing the estimated recipe, i.e. output of prep().\n\ntidy(pca_estimates)\n\n# A tibble: 3 × 6\n  number operation type      trained skip  id             \n   &lt;int&gt; &lt;chr&gt;     &lt;chr&gt;     &lt;lgl&gt;   &lt;lgl&gt; &lt;chr&gt;          \n1      1 step      rm        TRUE    FALSE rm_ebdbN       \n2      2 step      normalize TRUE    FALSE normalize_trXdA\n3      3 step      pca       TRUE    FALSE pca_aBb7l      \n\n\nIn this example, PCA is the 3rd step. By passing argument number = 3 when calling tidy(), you can extract loadings.\n\ntidy(pca_estimates, number = 3)\n\n# A tibble: 25 × 4\n   terms   value component id       \n   &lt;chr&gt;   &lt;dbl&gt; &lt;chr&gt;     &lt;chr&gt;    \n 1 X1     0.0761 PC1       pca_aBb7l\n 2 X2    -0.395  PC1       pca_aBb7l\n 3 X3     0.570  PC1       pca_aBb7l\n 4 X4    -0.560  PC1       pca_aBb7l\n 5 X5    -0.448  PC1       pca_aBb7l\n 6 X1     0.780  PC2       pca_aBb7l\n 7 X2     0.565  PC2       pca_aBb7l\n 8 X3     0.162  PC2       pca_aBb7l\n 9 X4    -0.197  PC2       pca_aBb7l\n10 X5     0.0864 PC2       pca_aBb7l\n# ℹ 15 more rows\n\n\nLet’s see entire loading matrix for this example.\n\ntidy(pca_estimates, number = 3) |&gt; \n  pivot_wider(names_from = component, values_from = value)\n\n# A tibble: 5 × 7\n  terms id            PC1     PC2       PC3      PC4     PC5\n  &lt;chr&gt; &lt;chr&gt;       &lt;dbl&gt;   &lt;dbl&gt;     &lt;dbl&gt;    &lt;dbl&gt;   &lt;dbl&gt;\n1 X1    pca_aBb7l  0.0761  0.780   0.000892  0.141    0.605 \n2 X2    pca_aBb7l -0.395   0.565  -0.295    -0.118   -0.651 \n3 X3    pca_aBb7l  0.570   0.162   0.241     0.638   -0.429 \n4 X4    pca_aBb7l -0.560  -0.197  -0.257     0.748    0.150 \n5 X5    pca_aBb7l -0.448   0.0864  0.888     0.00367 -0.0571\n\n\nBy additionally passing type = \"variance\" argument, you can see eigenvalues from the rows associated with terms == \"variance\", and derived statistics.\n\ntidy(pca_estimates, number = 3, type = \"variance\")\n\n# A tibble: 20 × 4\n   terms                          value component id       \n   &lt;chr&gt;                          &lt;dbl&gt;     &lt;int&gt; &lt;chr&gt;    \n 1 variance                      2.76           1 pca_aBb7l\n 2 variance                      1.61           2 pca_aBb7l\n 3 variance                      0.551          3 pca_aBb7l\n 4 variance                      0.0641         4 pca_aBb7l\n 5 variance                      0.0183         5 pca_aBb7l\n 6 cumulative variance           2.76           1 pca_aBb7l\n 7 cumulative variance           4.37           2 pca_aBb7l\n 8 cumulative variance           4.92           3 pca_aBb7l\n 9 cumulative variance           4.98           4 pca_aBb7l\n10 cumulative variance           5              5 pca_aBb7l\n11 percent variance             55.2            1 pca_aBb7l\n12 percent variance             32.1            2 pca_aBb7l\n13 percent variance             11.0            3 pca_aBb7l\n14 percent variance              1.28           4 pca_aBb7l\n15 percent variance              0.365          5 pca_aBb7l\n16 cumulative percent variance  55.2            1 pca_aBb7l\n17 cumulative percent variance  87.3            2 pca_aBb7l\n18 cumulative percent variance  98.4            3 pca_aBb7l\n19 cumulative percent variance  99.6            4 pca_aBb7l\n20 cumulative percent variance 100              5 pca_aBb7l",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Dimension reduction</span>"
    ]
  },
  {
    "objectID": "ch04_dimension_reduction.html#examples-4.12",
    "href": "ch04_dimension_reduction.html#examples-4.12",
    "title": "4  Dimension reduction",
    "section": "4.2 Examples 4.12",
    "text": "4.2 Examples 4.12\n\n4.2.1 Load data\n\ndat3 &lt;- read_csv(\"data/ch4_dat3.csv\")\n\nRows: 6 Columns: 4\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \",\"\ndbl (4): x1, x2, x3, y\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n\ndat3\n\n# A tibble: 6 × 4\n     x1    x2    x3     y\n  &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;\n1    -3    -3     5   -30\n2    -2    -3     7   -20\n3     0     0     4     0\n4     1     2     0     5\n5     2     2    -5    10\n6     2     2   -11    35\n\n\n\n\n4.2.2 Principal component regression (PCR)\nTo use principal component scores as input variables of a regression model, it is recommended to use workflow.\nFirst, let us define a recipe of data preprocessing including PCA. Let us create only two principal components, by passing argument num_comp = 2.\n\nrec_pca &lt;- \n  recipe(y ~ ., data = dat3) |&gt; \n  step_center(all_predictors()) |&gt; \n  step_pca(all_predictors(), num_comp = 2)\n\nLet us define a model. We will use just a default linear regression.\n\nlm_model &lt;- linear_reg()\n\nThen let us define a workflow.\n\npcr_wflow &lt;-\n  workflow() |&gt; \n  add_recipe(rec_pca) |&gt; \n  add_model(lm_model)\n\nFinally, we will train the workflow on training data.\n\npcr_fit &lt;- fit(pcr_wflow, data = dat3)\n\nSee the estimated coefficients on principal components.\n\ntidy(pcr_fit)\n\n# A tibble: 3 × 5\n  term         estimate std.error statistic p.value\n  &lt;chr&gt;           &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;   &lt;dbl&gt;\n1 (Intercept) -5.02e-15     3.58  -1.40e-15  1.00  \n2 PC1          2.92e+ 0     0.530  5.51e+ 0  0.0118\n3 PC2         -2.54e+ 0     2.35  -1.08e+ 0  0.360 \n\n\n\n\n4.2.3 Prediction\nLet us predict response variable value by using the estimated PCR model by calling predict() function.\n\ndat3 |&gt; \n  bind_cols(predict(pcr_fit, new_data = dat3))\n\n# A tibble: 6 × 5\n     x1    x2    x3     y  .pred\n  &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;  &lt;dbl&gt;\n1    -3    -3     5   -30 -23.2 \n2    -2    -3     7   -20 -24.6 \n3     0     0     4     0  -6.95\n4     1     2     0     5   7.57\n5     2     2    -5    10  18.4 \n6     2     2   -11    35  28.8",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Dimension reduction</span>"
    ]
  },
  {
    "objectID": "ch04_dimension_reduction.html#examples-4.14---4.15",
    "href": "ch04_dimension_reduction.html#examples-4.14---4.15",
    "title": "4  Dimension reduction",
    "section": "4.3 Examples 4.14 - 4.15",
    "text": "4.3 Examples 4.14 - 4.15\n\n4.3.1 Load data\n\ndat3 &lt;- read_csv(\"data/ch4_dat3.csv\")\n\nRows: 6 Columns: 4\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \",\"\ndbl (4): x1, x2, x3, y\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n\ndat3\n\n# A tibble: 6 × 4\n     x1    x2    x3     y\n  &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;\n1    -3    -3     5   -30\n2    -2    -3     7   -20\n3     0     0     4     0\n4     1     2     0     5\n5     2     2    -5    10\n6     2     2   -11    35\n\n\n\n\n4.3.2 Partial least squares regression\nEstimating partial least squares(PLS) model within tidymodels framework requires some additional settings. First, you need to install {mixOmics} package to use it as an engine. {mixOmics} exists not on CRAN but on Bioconductor repository, so you should follow the installation guidance. Then, using parsnip::pls() requires installing and loading an extension package {plsmod}. Please see an example.\nNow, let us define a PLS model. Pass scale = FALSE in set_engine() when you want to use original scale for predictor variables instead of standardizing them.\n\nlibrary(plsmod)\n\npls_model &lt;- \n  pls(num_comp = 2) |&gt; \n  set_engine(\"mixOmics\", scale = FALSE) |&gt; \n  set_mode(\"regression\")\n\nLet’s estimate the PLS model.\n\npls_fit &lt;- \n  pls_model |&gt; \n  fit(y ~ ., data = dat3)\n\n\n\n4.3.3 Estimated model\nExtracting each estimated matrix from PLS model mostly requires direct access to model engine object by extract_fit_engine(); not much convinient interface through tidymodels framework.\nSee latent matrix (T):\n\npls_fit |&gt; \n  extract_fit_engine() |&gt; \n  pluck(\"variates\", \"X\")\n\n       comp1      comp2\n1 -6.3186736 -2.0166999\n2 -7.8514212 -0.5903390\n3 -3.6285452  1.5267780\n4  0.9071363  1.9538097\n5  5.7243429  0.7083091\n6 11.1671608 -1.5818579\n\n\nX-loading matrix (P):\n\npls_fit |&gt; \n  extract_fit_engine() |&gt; \n  pluck(\"mat.c\")\n\n         [,1]      [,2]\nx1  0.2539947 0.5481649\nx2  0.2860734 0.7356762\nx3 -0.9248982 0.4238016\n\n\nLoading weight matrix (W):\n\npls_fit |&gt; \n  extract_fit_engine() |&gt; \n  pluck(\"loadings\", \"X\")\n\n        comp1     comp2\nx1  0.2815251 0.6510676\nx2  0.3128056 0.6321920\nx3 -0.9071363 0.4200527\n\n\nor, simply\n\ntidy(pls_fit)\n\n# A tibble: 8 × 4\n  term   value type       component\n  &lt;chr&gt;  &lt;dbl&gt; &lt;chr&gt;          &lt;dbl&gt;\n1 x1     0.282 predictors         1\n2 x1     0.651 predictors         2\n3 x2     0.313 predictors         1\n4 x2     0.632 predictors         2\n5 x3    -0.907 predictors         1\n6 x3     0.420 predictors         2\n7 Y      1     outcomes           1\n8 Y      1     outcomes           2\n\n\nPlease note that loadings for y are different from the book example.\nWeight matrix related to original predictors are\n\npls_fit |&gt; \n  extract_fit_engine() |&gt; \n  pluck(\"loadings.star\", 1)\n\n         [,1]      [,2]\nx1  0.2815251 0.6629719\nx2  0.3128056 0.6454189\nx3 -0.9071363 0.3816945\n\n\n\n\n4.3.4 Prediction\nPredicting expected response variable value is easy within tidymodels framework, by calling predict() function.\n\ndat3 |&gt; \n  bind_cols(predict(pls_fit, new_data = dat3))\n\n# A tibble: 6 × 5\n     x1    x2    x3     y  .pred\n  &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;  &lt;dbl&gt;\n1    -3    -3     5   -30 -23.5 \n2    -2    -3     7   -20 -24.5 \n3     0     0     4     0  -6.82\n4     1     2     0     5   7.52\n5     2     2    -5    10  18.5 \n6     2     2   -11    35  28.7",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Dimension reduction</span>"
    ]
  },
  {
    "objectID": "ch05_classification.html",
    "href": "ch05_classification.html",
    "title": "5  Classification",
    "section": "",
    "text": "5.1 Example 5.2",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Classification</span>"
    ]
  },
  {
    "objectID": "ch05_classification.html#example-5.2",
    "href": "ch05_classification.html#example-5.2",
    "title": "5  Classification",
    "section": "",
    "text": "5.1.1 Load data\nTo estimate classification models, you first need to convert outcome variable to be a factor, if it is a numeric in original data.\n\ndat1 &lt;- read_csv(\"data/ch7_dat1.csv\") |&gt; \n  mutate(class = as.factor(class))\n\nRows: 9 Columns: 4\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \",\"\ndbl (4): ID, X1, X2, class\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n\ndat1\n\n# A tibble: 9 × 4\n     ID    X1    X2 class\n  &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;fct&gt;\n1     1     5     7 1    \n2     2     4     3 2    \n3     3     7     8 2    \n4     4     8     6 2    \n5     5     3     6 1    \n6     6     2     5 1    \n7     7     6     6 1    \n8     8     9     6 2    \n9     9     5     4 2    \n\n\n\n\n5.1.2 Split data\nLet us divide data into two fold: training and testing data. Within tidymodels framework, {rsamples} package, a part of {tidymodels} provides functions to split data.\ninitial_split() randomly split data into training and testing data, while initial_time_split() take first rows of data as training and last rows of data as testing data. To be consistent with a book example, let us use initial_time_split() here.\nIn this example, let us use 8 data points as training data, and just one data point as testing data.\n\ndat1_split &lt;- initial_time_split(dat1, prop = 8 / 9)\ndat_train &lt;- training(dat1_split)\ndat_test &lt;- testing(dat1_split)\n\n\n\n\n\n\n\nNote\n\n\n\nA book examples used 7 data points as training data, but we are using 8 here due to a parameter setting mechanism in tidymodels framework. Within tidymodels framework, 8 data points are the minimum number of data points to use 3 neighborhoods. If you use 7 data points as training data, tidymodels will automatically reset the number of neighbors to be 2. For more details, please see https://parsnip.tidymodels.org/reference/details_nearest_neighbor_kknn.html.\n\n\n\n\n5.1.3 3-NN model estimation\nTidymodels framework uses {kknn} as an engine of k-nearest-neighbor models for both classification and regression. After installing {kknn} package, call nearest_neighbor() with neighbors argument to specify number of neighbors to use in estimation. You must specify mode of the model to be either \"classification\" or \"regression\". In this example, use \"classification\" mode.\n\nknn_model &lt;- \n  nearest_neighbor(neighbors = 3) |&gt; \n  set_engine(\"kknn\", scale = FALSE) |&gt; \n  set_mode(\"classification\")\n\nThen, estimate the model with fit() function.\n\nknn_fit &lt;- \n  knn_model |&gt; \n  fit(class ~ X1 + X2, data = dat_train)\n\nLet us check estimated results on training data.\n\ndat_train |&gt; \n  bind_cols(\n    knn_fit |&gt; \n      extract_fit_engine() |&gt; \n      pluck(\"fitted.values\") |&gt; \n      set_names(\"estimated_class\")\n  )\n\n# A tibble: 8 × 5\n     ID    X1    X2 class estimated_class\n  &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;fct&gt; &lt;fct&gt;          \n1     1     5     7 1     1              \n2     2     4     3 2     1              \n3     3     7     8 2     1              \n4     4     8     6 2     2              \n5     5     3     6 1     1              \n6     6     2     5 1     1              \n7     7     6     6 1     1              \n8     8     9     6 2     2              \n\n\n\n\n5.1.4 Prediction\nLet us make a prediction on testing data, by calling predict() function.\n\ndat_test |&gt; \n  bind_cols(\n    predict(knn_fit, new_data = dat_test)\n  )\n\n# A tibble: 1 × 5\n     ID    X1    X2 class .pred_class\n  &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;fct&gt; &lt;fct&gt;      \n1     9     5     4 2     2",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Classification</span>"
    ]
  },
  {
    "objectID": "ch05_classification.html#examples-5.3---5.4",
    "href": "ch05_classification.html#examples-5.3---5.4",
    "title": "5  Classification",
    "section": "5.2 Examples 5.3 - 5.4",
    "text": "5.2 Examples 5.3 - 5.4\n\n5.2.1 Load data\n\ndat3 &lt;- read_csv(\"data/ch5_dat3.csv\")\n\nRows: 9 Columns: 4\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \",\"\nchr (1): gender\ndbl (3): ID, age_gr, class\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n\ndat3\n\n# A tibble: 9 × 4\n     ID gender age_gr class\n  &lt;dbl&gt; &lt;chr&gt;   &lt;dbl&gt; &lt;dbl&gt;\n1     1 M           2     1\n2     2 M           2     2\n3     3 M           3     1\n4     4 M           4     1\n5     5 F           1     1\n6     6 F           2     2\n7     7 F           2     1\n8     8 F           3     2\n9     9 F           4     2\n\n\n\n\n5.2.2 Convert data type\nFirst, convert output to be a factor, because you are going to build a classification model. Please use typical data wrangling instead of using recipe when transforming output variable.\n\ndat3 &lt;- \n  dat3 |&gt; \n  mutate(class = as.factor(class))\n\nDefine a recipe to convert categorical input variable into factors by using recipe. When original variable type is numeric, use step_num2factor(). When original variable type is character, use step_string2factor().\n\nnb_rec &lt;- \n  recipe(class ~ gender + age_gr, data = dat3) |&gt; \n  step_num2factor(age_gr, levels = c(\"1\", \"2\", \"3\", \"4\")) |&gt; \n  step_string2factor(gender)\n\n\n\n\n\n\n\nNote\n\n\n\nIf you include output variable type conversion within a recipe and include it in a workflow, predict() will return an error because it removes output variable from new_data before applying a recipe.\n\n\n\n\n5.2.3 Ex 5.3: Naive bayes clssification\n\n5.2.3.1 Estimate a classifier\nLet us use parsnip::naive_Bayes() to define naive bayes model. By default, it uses {klaR} engine and requires installation and loading {discrim} package to fit the model. Please find more details here.\n\nlibrary(discrim)\n\n\nAttaching package: 'discrim'\n\n\nThe following object is masked from 'package:dials':\n\n    smoothness\n\nnb_model &lt;- naive_Bayes(smoothness = 0) |&gt; \n  set_engine(\"klaR\") # use {klaR} engine; this is default engine\n\nYou can check engine-level model fitting function template by calling translate() function.\n\nnb_model |&gt; translate()\n\nNaive Bayes Model Specification (classification)\n\nMain Arguments:\n  smoothness = 0\n\nComputational engine: klaR \n\nModel fit template:\ndiscrim::klar_bayes_wrapper(x = missing_arg(), y = missing_arg(), \n    adjust = 0, usekernel = TRUE)\n\n\nLet us define workflow and train the model.\n\nnb_fit &lt;- \n  workflow() |&gt; \n  add_recipe(nb_rec) |&gt; \n  add_model(nb_model) |&gt; \n  fit(dat3)\n\nnb_fit\n\n══ Workflow [trained] ══════════════════════════════════════════════════════════\nPreprocessor: Recipe\nModel: naive_Bayes()\n\n── Preprocessor ────────────────────────────────────────────────────────────────\n2 Recipe Steps\n\n• step_num2factor()\n• step_string2factor()\n\n── Model ───────────────────────────────────────────────────────────────────────\n$apriori\ngrouping\n        1         2 \n0.5555556 0.4444444 \n\n$tables\n$tables$gender\n        var\ngrouping    F    M\n       1 0.40 0.60\n       2 0.75 0.25\n\n$tables$age_gr\n        var\ngrouping    1    2    3    4\n       1 0.20 0.40 0.20 0.20\n       2 0.00 0.50 0.25 0.25\n\n\n$levels\n[1] \"1\" \"2\"\n\n$call\nNaiveBayes.default(x = ~maybe_data_frame(x), grouping = ~y, usekernel = ~TRUE, \n    adjust = ~0)\n\n$x\n  gender age_gr\n1      M      2\n2      M      2\n3      M      3\n4      M      4\n5      F      1\n6      F      2\n7      F      2\n8      F      3\n9      F      4\n\n$usekernel\n[1] TRUE\n\n$varnames\n[1] \"gender\" \"age_gr\"\n\nattr(,\"class\")\n[1] \"NaiveBayes\"\n\n\n\n\n5.2.3.2 Prediction\nLet us predict posterior probabilities and predict class as well. Both can by done by calling predict() function, where you can specify type = \"prob\" for posterior probability prediction and type = \"class\" for class prediction.\n\nresults &lt;- dat3 |&gt; \n  bind_cols(\n    predict(nb_fit, new_data = dat3, type = \"prob\"),\n    predict(nb_fit, new_data = dat3, type = \"class\")\n  )\n\nresults\n\n# A tibble: 9 × 7\n     ID gender age_gr class .pred_1 .pred_2 .pred_class\n  &lt;dbl&gt; &lt;chr&gt;   &lt;dbl&gt; &lt;fct&gt;   &lt;dbl&gt;   &lt;dbl&gt; &lt;fct&gt;      \n1     1 M           2 1       0.706 0.294   1          \n2     2 M           2 2       0.706 0.294   1          \n3     3 M           3 1       0.706 0.294   1          \n4     4 M           4 1       0.706 0.294   1          \n5     5 F           1 1       0.993 0.00744 1          \n6     6 F           2 2       0.348 0.652   2          \n7     7 F           2 1       0.348 0.652   2          \n8     8 F           3 2       0.348 0.652   2          \n9     9 F           4 2       0.348 0.652   2          \n\n\n\n\n5.2.3.3 Use {naivebayes} engine\nInstead of default engine {klaR}, let us use different engine from {naivebayes} package. The only change that you need to make is engine name argument in set_engine() when defining a model. It provides an identical results.\n\nlibrary(discrim)\n\nnb_model &lt;- naive_Bayes(smoothness = 0) |&gt; \n  set_engine(\"naivebayes\") # use {naivebayes} engine\n\nnb_model |&gt; translate()\n\nNaive Bayes Model Specification (classification)\n\nMain Arguments:\n  smoothness = 0\n\nComputational engine: naivebayes \n\nModel fit template:\nnaivebayes::naive_bayes(x = missing_arg(), y = missing_arg(), \n    adjust = 0, usekernel = TRUE)\n\nnb_fit &lt;- \n  workflow() |&gt; \n  add_recipe(nb_rec) |&gt; \n  add_model(nb_model) |&gt; \n  fit(dat3)\n\nWarning: naive_bayes(): Feature age_gr - zero probabilities are present.\nConsider Laplace smoothing.\n\nnb_fit\n\n══ Workflow [trained] ══════════════════════════════════════════════════════════\nPreprocessor: Recipe\nModel: naive_Bayes()\n\n── Preprocessor ────────────────────────────────────────────────────────────────\n2 Recipe Steps\n\n• step_num2factor()\n• step_string2factor()\n\n── Model ───────────────────────────────────────────────────────────────────────\n\n================================= Naive Bayes ==================================\n\nCall:\nnaive_bayes.default(x = maybe_data_frame(x), y = y, usekernel = TRUE, \n    adjust = ~0)\n\n-------------------------------------------------------------------------------- \n \nLaplace smoothing: 0\n\n-------------------------------------------------------------------------------- \n \nA priori probabilities: \n\n        1         2 \n0.5555556 0.4444444 \n\n-------------------------------------------------------------------------------- \n \nTables: \n\n-------------------------------------------------------------------------------- \n:: gender (Bernoulli) \n-------------------------------------------------------------------------------- \n      \ngender    1    2\n     F 0.40 0.75\n     M 0.60 0.25\n\n-------------------------------------------------------------------------------- \n:: age_gr (Categorical) \n-------------------------------------------------------------------------------- \n      \nage_gr    1    2\n     1 0.20 0.00\n     2 0.40 0.50\n     3 0.20 0.25\n     4 0.20 0.25\n\n--------------------------------------------------------------------------------\n\nresults &lt;- dat3 |&gt; \n  bind_cols(\n    predict(nb_fit, new_data = dat3, type = \"prob\"),\n    predict(nb_fit, new_data = dat3, type = \"class\")\n  )\n\nresults\n\n# A tibble: 9 × 7\n     ID gender age_gr class .pred_1 .pred_2 .pred_class\n  &lt;dbl&gt; &lt;chr&gt;   &lt;dbl&gt; &lt;fct&gt;   &lt;dbl&gt;   &lt;dbl&gt; &lt;fct&gt;      \n1     1 M           2 1       0.706 0.294   1          \n2     2 M           2 2       0.706 0.294   1          \n3     3 M           3 1       0.706 0.294   1          \n4     4 M           4 1       0.706 0.294   1          \n5     5 F           1 1       0.993 0.00744 1          \n6     6 F           2 2       0.348 0.652   2          \n7     7 F           2 1       0.348 0.652   2          \n8     8 F           3 2       0.348 0.652   2          \n9     9 F           4 2       0.348 0.652   2          \n\n\n\n\n\n5.2.4 Ex 5.4: Performance evaluation\n{yardstick}, which is a part of {tidymodels} framework, provides functions to evaluate performance of classification and regression models. Here, let us take a look at several classification performance metrics.\n\n5.2.4.1 Confusion matrix\nCreate confusion matrix by calling conf_mat() with classification result data frame. For other required arguments, truth represents a column name of actual class labels and estimate represents a column name of predicted class labels.\n\nresults |&gt; \n  conf_mat(truth = class, estimate = .pred_class)\n\n          Truth\nPrediction 1 2\n         1 4 1\n         2 1 3\n\n\n\n\n5.2.4.2 Accuracy\nEvaluate simple classification accuracy by accuracy()\n\nresults |&gt; \n  accuracy(truth = class, estimate = .pred_class)\n\n# A tibble: 1 × 3\n  .metric  .estimator .estimate\n  &lt;chr&gt;    &lt;chr&gt;          &lt;dbl&gt;\n1 accuracy binary         0.778\n\n\n\n\n5.2.4.3 Sensitivity\nUse sens(). Pass event_level argument if needs to explicitly define “event” class label. It argument is only applicable to binary classification, and the value should be either \"first\" or \"second\".\n\nresults |&gt; \n  sens(truth = class, estimate = .pred_class, event_level = \"first\")\n\n# A tibble: 1 × 3\n  .metric .estimator .estimate\n  &lt;chr&gt;   &lt;chr&gt;          &lt;dbl&gt;\n1 sens    binary           0.8\n\n\n\n\n5.2.4.4 Specificity\nUse spec().\n\nresults |&gt; \n  spec(truth = class, estimate = .pred_class, event_level = \"first\")\n\n# A tibble: 1 × 3\n  .metric .estimator .estimate\n  &lt;chr&gt;   &lt;chr&gt;          &lt;dbl&gt;\n1 spec    binary          0.75\n\n\n\n\n5.2.4.5 F1-score\nUse f_meas()\n\nresults |&gt; \n  f_meas(truth = class, estimate = .pred_class, event_level = \"first\")\n\n# A tibble: 1 × 3\n  .metric .estimator .estimate\n  &lt;chr&gt;   &lt;chr&gt;          &lt;dbl&gt;\n1 f_meas  binary           0.8\n\n\n\n\n5.2.4.6 Evaluate multiple metrics\n{yardstick} provides a convenient way to evaluate multiple metrics.\nFirst, create a set of metrics to compute by calling metric_set() with metric functions of interest as arguments.\n\nmultiple_metrics &lt;- metric_set(accuracy, sens, spec, f_meas)\n\nLet’s print the object.\n\nmultiple_metrics\n\nA metric set, consisting of:\n- `accuracy()`, a class metric | direction: maximize\n- `sens()`, a class metric     | direction: maximize\n- `spec()`, a class metric     | direction: maximize\n- `f_meas()`, a class metric   | direction: maximize\n\n\nBy looking at classes and a type of multiple_metrics object, you can see that it is a callable function.\n\nclass(multiple_metrics)\n\n[1] \"class_prob_metric_set\" \"metric_set\"            \"function\"             \n\ntypeof(multiple_metrics)\n\n[1] \"closure\"\n\n\nNow, call the function to compute multiple metrics of interest.\n\nresults |&gt; \n  multiple_metrics(truth = class, estimate = .pred_class, event_level = \"first\")\n\n# A tibble: 4 × 3\n  .metric  .estimator .estimate\n  &lt;chr&gt;    &lt;chr&gt;          &lt;dbl&gt;\n1 accuracy binary         0.778\n2 sens     binary         0.8  \n3 spec     binary         0.75 \n4 f_meas   binary         0.8",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Classification</span>"
    ]
  },
  {
    "objectID": "ch06_logistic_regression.html",
    "href": "ch06_logistic_regression.html",
    "title": "6  Logistic regression",
    "section": "",
    "text": "6.1 Examples 6.1, 6.3",
    "crumbs": [
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Logistic regression</span>"
    ]
  },
  {
    "objectID": "ch06_logistic_regression.html#examples-6.1-6.3",
    "href": "ch06_logistic_regression.html#examples-6.1-6.3",
    "title": "6  Logistic regression",
    "section": "",
    "text": "6.1.1 Load data\n\ndat1 &lt;- read_csv(\"data/ch6_dat1.csv\")\n\nRows: 15 Columns: 4\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \",\"\nchr (1): Class\ndbl (3): Break, Sleep, Circle\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n\n\nLogistic regression requires outcome variable to be a factor.\n\ndat1 &lt;-\n  dat1 |&gt; \n  mutate(Class = factor(Class, levels = c(\"Average\", \"Excellent\")))\n\n\n\n6.1.2 Ex 6.1: Logistic regression model\nDefine a model type to be logistic regression, by calling logistic_reg(). By default, it uses {glm} engine and \"classification\" mode.\n\nlogistic_model &lt;- logistic_reg()\n\nLet us train the model by using dat1 data. Let us use workflow pipeline. Because we do not have a recipe, we will add formula instead.\n\nlogistic_fit &lt;- \n  workflow() |&gt; \n  add_formula(Class ~ Break + Sleep + Circle) |&gt; \n  add_model(logistic_model) |&gt; \n  fit(dat1)\n\nlogistic_fit\n\n══ Workflow [trained] ══════════════════════════════════════════════════════════\nPreprocessor: Formula\nModel: logistic_reg()\n\n── Preprocessor ────────────────────────────────────────────────────────────────\nClass ~ Break + Sleep + Circle\n\n── Model ───────────────────────────────────────────────────────────────────────\n\nCall:  stats::glm(formula = ..y ~ ., family = stats::binomial, data = data)\n\nCoefficients:\n(Intercept)        Break        Sleep       Circle  \n    -30.511        2.031        3.471        2.414  \n\nDegrees of Freedom: 14 Total (i.e. Null);  11 Residual\nNull Deviance:      20.19 \nResidual Deviance: 10.42    AIC: 18.42\n\n\nCheck statistics for each coefficients.\n\ntidy(logistic_fit)\n\n# A tibble: 4 × 5\n  term        estimate std.error statistic p.value\n  &lt;chr&gt;          &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;   &lt;dbl&gt;\n1 (Intercept)   -30.5      18.0      -1.69  0.0904\n2 Break           2.03      1.98      1.02  0.306 \n3 Sleep           3.47      2.07      1.67  0.0944\n4 Circle          2.41      1.40      1.73  0.0838\n\n\nMake a prediction by calling predict() function.\n\ndat1 |&gt; \n  bind_cols(\n    logit = predict(logistic_fit, new_data = dat1, type = \"raw\"),\n    predict(logistic_fit, new_data = dat1, type = \"prob\"),\n    predict(logistic_fit, new_data = dat1, type = \"class\")\n  )\n\n# A tibble: 15 × 8\n   Break Sleep Circle Class      logit .pred_Average .pred_Excellent .pred_class\n   &lt;dbl&gt; &lt;dbl&gt;  &lt;dbl&gt; &lt;fct&gt;      &lt;dbl&gt;         &lt;dbl&gt;           &lt;dbl&gt; &lt;fct&gt;      \n 1     0     8      2 Excellent  2.08         0.111         0.889    Excellent  \n 2     1     7      1 Excellent -1.77         0.855         0.145    Average    \n 3     0     9      0 Excellent  0.725        0.326         0.674    Excellent  \n 4     1     6      4 Excellent  2.00         0.119         0.881    Excellent  \n 5     1     8      2 Excellent  4.11         0.0161        0.984    Excellent  \n 6     0     7      3 Excellent  1.03         0.264         0.736    Excellent  \n 7     0     7      0 Average   -6.22         0.998         0.00199  Average    \n 8     1     6      1 Average   -5.24         0.995         0.00527  Average    \n 9     0     7      2 Average   -1.39         0.800         0.200    Average    \n10     0     8      1 Average   -0.331        0.582         0.418    Average    \n11     0     5      2 Average   -8.33         1.00          0.000241 Average    \n12     1     8      0 Average   -0.714        0.671         0.329    Average    \n13     0     6      3 Average   -2.44         0.920         0.0799   Average    \n14     1     7      2 Average    0.644        0.344         0.656    Excellent  \n15     0     6      1 Average   -7.27         0.999         0.000694 Average",
    "crumbs": [
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Logistic regression</span>"
    ]
  }
]