[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Data mining with {tidymodels}",
    "section": "",
    "text": "1 Preface\nThis is a book to convert example code on https://youngroklee-ml.github.io/data-mining-techniques/ into the {tidymodels} framework.",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Preface</span>"
    ]
  },
  {
    "objectID": "index.html#tidymodels",
    "href": "index.html#tidymodels",
    "title": "Data mining with {tidymodels}",
    "section": "1.1 Tidymodels",
    "text": "1.1 Tidymodels\nThe tidymodels framework is a collection of packages to provide intuitive and unified interface for modeling and machine learning. To get more information, please see the following materials:\n\ntidymodels website: https://www.tidymodels.org\n“Tidy modeling with R” by Max Kuhn and Julia Silge: https://www.tmwr.org",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Preface</span>"
    ]
  },
  {
    "objectID": "index.html#model-engine-packages",
    "href": "index.html#model-engine-packages",
    "title": "Data mining with {tidymodels}",
    "section": "1.2 Model engine packages",
    "text": "1.2 Model engine packages\nEach modeling packages need to be separately installed to use it within the tidymodels framework. Example codes in this book require the following R packages.\n\nlibrary(glmnet)\nlibrary(mixOmics)",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Preface</span>"
    ]
  },
  {
    "objectID": "ch02_regression.html",
    "href": "ch02_regression.html",
    "title": "2  Regression",
    "section": "",
    "text": "2.1 Examples 2.3 - 2.5, 2.7, 2.10 - 2.11",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Regression</span>"
    ]
  },
  {
    "objectID": "ch02_regression.html#examples-2.3---2.5-2.7-2.10---2.11",
    "href": "ch02_regression.html#examples-2.3---2.5-2.7-2.10---2.11",
    "title": "2  Regression",
    "section": "",
    "text": "2.1.1 Load data\n\ndat1 &lt;- read_csv(\"data/ch2_reg1.csv\")\n\nRows: 10 Columns: 4\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \",\"\ndbl (4): ID, age, height, weight\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n\nprint(dat1)\n\n# A tibble: 10 × 4\n      ID   age height weight\n   &lt;dbl&gt; &lt;dbl&gt;  &lt;dbl&gt;  &lt;dbl&gt;\n 1     1    21    170     60\n 2     2    47    167     65\n 3     3    36    173     67\n 4     4    15    165     54\n 5     5    54    168     73\n 6     6    25    177     71\n 7     7    32    169     68\n 8     8    18    172     62\n 9     9    43    171     66\n10    10    28    175     68\n\n\n\n\n2.1.2 Ex 2.3: Estimate coefficients\nUse {parsnip} package, a part of {tidymodels}, that provide a unified modeling interface.\nDefine a model type and engine.\n\nmodel &lt;- \n  linear_reg() |&gt;\n  set_engine(\"lm\") # \"lm\" is a default engine for `linear_reg()`\n\nEstimate the model by calling fit() function with formula and training data.\n\nmodel_fit &lt;- \n  model |&gt; \n  fit(weight ~ age + height, data = dat1)\n\nLet’s print estimation results.\n\nprint(model_fit)\n\nparsnip model object\n\n\nCall:\nstats::lm(formula = weight ~ age + height, data = data)\n\nCoefficients:\n(Intercept)          age       height  \n  -108.1672       0.3291       0.9553  \n\n\nbroom::tidy() can extract coefficient statistics as data frame, including estimates and test statistics. Column estimate represents the coefficient estimates.\n\ntidy(model_fit)\n\n# A tibble: 3 × 5\n  term        estimate std.error statistic p.value\n  &lt;chr&gt;          &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;   &lt;dbl&gt;\n1 (Intercept) -108.      42.1        -2.57 0.0371 \n2 age            0.329    0.0692      4.75 0.00208\n3 height         0.955    0.244       3.91 0.00579\n\n\n\n\n2.1.3 Ex 2.4: Estimate variance of error term\nbroom::glance() provides model-level statistics.\n\nglance(model_fit)\n\n# A tibble: 1 × 12\n  r.squared adj.r.squared sigma statistic p.value    df logLik   AIC   BIC\n      &lt;dbl&gt;         &lt;dbl&gt; &lt;dbl&gt;     &lt;dbl&gt;   &lt;dbl&gt; &lt;dbl&gt;  &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;\n1     0.822         0.771  2.65      16.1 0.00239     2  -22.2  52.3  53.5\n# ℹ 3 more variables: deviance &lt;dbl&gt;, df.residual &lt;int&gt;, nobs &lt;int&gt;\n\n\nsigma is the estimate of standard deviation of the error term, so just square it to estimate the variance of the error term.\n\nglance(model_fit)[[\"sigma\"]]^2\n\n[1] 7.038464\n\n\n\n\n2.1.4 Ex 2.5: Test a model\nCall extract_fit_engine() when you need to explicitly use underlying lm object.\n\nextract_fit_engine(model_fit)\n\n\nCall:\nstats::lm(formula = weight ~ age + height, data = data)\n\nCoefficients:\n(Intercept)          age       height  \n  -108.1672       0.3291       0.9553  \n\n\nThis is important when a function that you call require the underlying engine’s object, not tidymodels framework’s wrapper class. anova() function to conduct ANOVA test is one of such functions that you need to pass lm object.\n\nmodel_fit |&gt; \n  extract_fit_engine() |&gt; \n  anova()\n\nAnalysis of Variance Table\n\nResponse: weight\n          Df  Sum Sq Mean Sq F value   Pr(&gt;F)   \nage        1 119.299 119.299   16.95 0.004476 **\nheight     1 107.831 107.831   15.32 0.005793 **\nResiduals  7  49.269   7.038                    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\n\n\n2.1.5 Ex 2.7: Test each coefficient\nAs seen above, tidy() function returns data frame that contain test statistics.\n\ntidy(model_fit)\n\n# A tibble: 3 × 5\n  term        estimate std.error statistic p.value\n  &lt;chr&gt;          &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;   &lt;dbl&gt;\n1 (Intercept) -108.      42.1        -2.57 0.0371 \n2 age            0.329    0.0692      4.75 0.00208\n3 height         0.955    0.244       3.91 0.00579\n\n\n\n\n2.1.6 Ex 2.10: Mean prediciton\nLet’s create a new data set as a tidyverse data frame tibble.\n\nnewdata &lt;- tibble(age = 40, height = 170)\n\nMake a mean prediction by calling predict() function, and append it to the data set as a column by calling bind_cols().\n\nnewdata |&gt; \n  bind_cols(predict(model_fit, new_data = newdata))\n\n# A tibble: 1 × 3\n    age height .pred\n  &lt;dbl&gt;  &lt;dbl&gt; &lt;dbl&gt;\n1    40    170  67.4\n\n\nAlso, add 95% confidence interval by calling predict() function with argument type = \"conf_int\".\n\nnewdata |&gt; \n  bind_cols(predict(model_fit, new_data = newdata)) |&gt; \n  bind_cols(predict(model_fit, new_data = newdata, type = \"conf_int\"))\n\n# A tibble: 1 × 5\n    age height .pred .pred_lower .pred_upper\n  &lt;dbl&gt;  &lt;dbl&gt; &lt;dbl&gt;       &lt;dbl&gt;       &lt;dbl&gt;\n1    40    170  67.4        65.0        69.8\n\n\n\n\n2.1.7 Ex 2.11 Prediction interval\nInstead of confidence interval, you can add prediction interval by passing argument type = \"pred_int\".\n\nnewdata |&gt; \n  bind_cols(predict(model_fit, new_data = newdata)) |&gt; \n  bind_cols(predict(model_fit, new_data = newdata, type = \"pred_int\"))\n\n# A tibble: 1 × 5\n    age height .pred .pred_lower .pred_upper\n  &lt;dbl&gt;  &lt;dbl&gt; &lt;dbl&gt;       &lt;dbl&gt;       &lt;dbl&gt;\n1    40    170  67.4        60.7        74.1",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Regression</span>"
    ]
  },
  {
    "objectID": "ch02_regression.html#examples-2.14-2.16",
    "href": "ch02_regression.html#examples-2.14-2.16",
    "title": "2  Regression",
    "section": "2.2 Examples 2.14, 2.16",
    "text": "2.2 Examples 2.14, 2.16\n\n2.2.1 Load data\n\ndat1 &lt;- read_csv(\"data/ch2_coil.csv\")\n\nRows: 10 Columns: 3\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \",\"\ndbl (3): temp, thick, y\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n\n\n\n\n2.2.2 Ex 2.14: Indicator variable\n\n2.2.2.1 Set indicator variable\nDefine feature engineering steps with {recipes} package, a part of {tidymodels}.\nFirst, call recipe() to define input variables, output variable, and data.\n\nrec &lt;- \n  recipe(y ~ temp + thick, data = dat1)\n\nSee the recipe recognizes that there are one outcome variable and two predictor variables.\n\nrec\n\n\n\n\n── Recipe ──────────────────────────────────────────────────────────────────────\n\n\n\n\n\n── Inputs \n\n\nNumber of variables by role\n\n\noutcome:   1\npredictor: 2\n\n\nYou want to consider thick as a categorical variable by using factor() inside step_mutate() and create a dummy variable by adding step_dummy().\n\nrec &lt;- \n  rec |&gt; \n  step_mutate(thick = factor(thick, levels = c(6, 2))) |&gt; \n  step_dummy(thick)\n\nNow check that the updated recipe recognize thick as a dummy variable.\n\nrec\n\n\n\n\n── Recipe ──────────────────────────────────────────────────────────────────────\n\n\n\n\n\n── Inputs \n\n\nNumber of variables by role\n\n\noutcome:   1\npredictor: 2\n\n\n\n\n\n── Operations \n\n\n• Variable mutation for: factor(thick, levels = c(6, 2))\n\n\n• Dummy variables from: thick\n\n\nOf course, you can make write it as one pipeline\n\nrec &lt;- \n  recipe(y ~ temp + thick, data = dat1) |&gt; \n  step_mutate(thick = factor(thick, levels = c(6, 2))) |&gt; \n  step_dummy(thick)\n\nQuickly see what the output would look like. Here, prep() is a function to train feature engineering, and juice() is a result of applying the feature engineering to training data.\n\nrec |&gt; \n  prep() |&gt; \n  juice()\n\n# A tibble: 10 × 3\n    temp     y thick_X2\n   &lt;dbl&gt; &lt;dbl&gt;    &lt;dbl&gt;\n 1   540  52.5        1\n 2   660  50.2        1\n 3   610  51.3        1\n 4   710  49.1        1\n 5   570  50.8        0\n 6   700  48.7        0\n 7   560  51.2        0\n 8   600  50.8        0\n 9   680  49.3        0\n10   530  51.5        0\n\n\n\n\n2.2.2.2 Include feature engineering into a workflow\nLet’s start with defining model type and engine as we did in the first example above.\n\nmodel &lt;- \n  linear_reg() |&gt; \n  set_engine(\"lm\")\n\nNow, let’s define a “workflow” from {workflows} package, a part of {tidymodels}, to combine a recipe and a model.\n\nwflow &lt;- \n  workflow() |&gt; \n  add_recipe(rec) |&gt; \n  add_model(model) \n\nThe workflow recognizes that rec is a preprocessor before estimating a regression model.\n\nwflow\n\n══ Workflow ════════════════════════════════════════════════════════════════════\nPreprocessor: Recipe\nModel: linear_reg()\n\n── Preprocessor ────────────────────────────────────────────────────────────────\n2 Recipe Steps\n\n• step_mutate()\n• step_dummy()\n\n── Model ───────────────────────────────────────────────────────────────────────\nLinear Regression Model Specification (regression)\n\nComputational engine: lm \n\n\nNow, call fit() function by passing training data to estimate a regression model.\n\nlm_model &lt;- \n  wflow |&gt; \n  fit(dat1)\n\nSee the model estimation results.\n\nlm_model\n\n══ Workflow [trained] ══════════════════════════════════════════════════════════\nPreprocessor: Recipe\nModel: linear_reg()\n\n── Preprocessor ────────────────────────────────────────────────────────────────\n2 Recipe Steps\n\n• step_mutate()\n• step_dummy()\n\n── Model ───────────────────────────────────────────────────────────────────────\n\nCall:\nstats::lm(formula = ..y ~ ., data = data)\n\nCoefficients:\n(Intercept)         temp     thick_X2  \n   61.10797     -0.01768      0.80415  \n\n\nYou can still use broom::tidy() function to return coefficient-level statistics.\n\ntidy(lm_model)\n\n# A tibble: 3 × 5\n  term        estimate std.error statistic  p.value\n  &lt;chr&gt;          &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;    &lt;dbl&gt;\n1 (Intercept)  61.1      0.703       86.9  7.06e-12\n2 temp         -0.0177   0.00115    -15.4  1.18e- 6\n3 thick_X2      0.804    0.150        5.38 1.03e- 3\n\n\n\n\n\n2.2.3 Ex 2.16: Interaction\nYou can revise the workflow by updating recipe to include interaction term.\n\n2.2.3.1 Update recipe\nAdd step_interact() to existing recipe to add interaction term. Because a dummy variable will be used in this step, use starts_with(\"thick\") to capture dummy variable names, instead of using original variable name.\n\nrec_interaction &lt;-\n  rec |&gt; \n  step_interact(terms = ~ temp:starts_with(\"thick\"))\n\n\nrec_interaction\n\n\n\n\n── Recipe ──────────────────────────────────────────────────────────────────────\n\n\n\n\n\n── Inputs \n\n\nNumber of variables by role\n\n\noutcome:   1\npredictor: 2\n\n\n\n\n\n── Operations \n\n\n• Variable mutation for: factor(thick, levels = c(6, 2))\n\n\n• Dummy variables from: thick\n\n\n• Interactions with: temp:starts_with(\"thick\")\n\n\nAgain, quickly check a resulting training data.\n\nrec_interaction |&gt; \n  prep() |&gt; \n  juice()\n\n# A tibble: 10 × 4\n    temp     y thick_X2 temp_x_thick_X2\n   &lt;dbl&gt; &lt;dbl&gt;    &lt;dbl&gt;           &lt;dbl&gt;\n 1   540  52.5        1             540\n 2   660  50.2        1             660\n 3   610  51.3        1             610\n 4   710  49.1        1             710\n 5   570  50.8        0               0\n 6   700  48.7        0               0\n 7   560  51.2        0               0\n 8   600  50.8        0               0\n 9   680  49.3        0               0\n10   530  51.5        0               0\n\n\n\n\n2.2.3.2 Update workflow\nUpdate workflow by calling update_recipe().\n\nwflow_interaction &lt;- \n  wflow |&gt; \n  update_recipe(rec_interaction)\n\n\nwflow_interaction\n\n══ Workflow ════════════════════════════════════════════════════════════════════\nPreprocessor: Recipe\nModel: linear_reg()\n\n── Preprocessor ────────────────────────────────────────────────────────────────\n3 Recipe Steps\n\n• step_mutate()\n• step_dummy()\n• step_interact()\n\n── Model ───────────────────────────────────────────────────────────────────────\nLinear Regression Model Specification (regression)\n\nComputational engine: lm \n\n\n\n\n2.2.3.3 Estimate a model with the new workflow\nCall fit() function with the new workflow.\n\nlm_model_interaction &lt;- \n  wflow_interaction |&gt; \n  fit(dat1)\n\nSee estimated coefficients.\n\ntidy(lm_model_interaction)\n\n# A tibble: 4 × 5\n  term            estimate std.error statistic  p.value\n  &lt;chr&gt;              &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;    &lt;dbl&gt;\n1 (Intercept)     60.1       0.750       80.2  2.53e-10\n2 temp            -0.0161    0.00123    -13.1  1.23e- 5\n3 thick_X2         3.28      1.21         2.71 3.52e- 2\n4 temp_x_thick_X2 -0.00399   0.00194     -2.05 8.57e- 2",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Regression</span>"
    ]
  },
  {
    "objectID": "ch03_regularized_regression.html",
    "href": "ch03_regularized_regression.html",
    "title": "3  Regularized regression",
    "section": "",
    "text": "3.1 Examples 3.1 - 3.2",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Regularized regression</span>"
    ]
  },
  {
    "objectID": "ch03_regularized_regression.html#examples-3.1---3.2",
    "href": "ch03_regularized_regression.html#examples-3.1---3.2",
    "title": "3  Regularized regression",
    "section": "",
    "text": "3.1.1 Load data\n\ndat1 &lt;- read_csv(\"data/ch3_dat1.csv\")\n\nRows: 7 Columns: 3\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \",\"\ndbl (3): x1, x2, y\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n\n\nNumber of objects\n\nN &lt;- nrow(dat1)\nN\n\n[1] 7\n\n\n\n\n3.1.2 Standardize input variables\nDefine a recipe to standardize each input variable\n\nrec &lt;- \n  recipe(y ~ x1 + x2, data = dat1) |&gt; \n  step_normalize(x1, x2)\n\n\n\n3.1.3 Ex 3.1: Lasso\n\n3.1.3.1 Basics\nRegularized regression model is still defined by linear_reg(), but with additional arguments penalty and mixture, where penalty is for the amount of regularization, while mixture is for proportion of L1 regularization. Set mixture = 1 to be Lasso. And let us set penalty = 0 to check a regression coefficient without regularization.\nDefault engine \"lm\" does not support regularized regression, so you should set a specific engine that support regularized regression. Let us use \"glmnet\" here.\n\nlasso_model &lt;- \n  linear_reg(penalty = 0, mixture = 1) |&gt; \n  set_engine(\"glmnet\")\n\nThis is translated into {glmnet} syntax like below:\n\nlasso_model |&gt; \n  translate()\n\nLinear Regression Model Specification (regression)\n\nMain Arguments:\n  penalty = 0\n  mixture = 1\n\nComputational engine: glmnet \n\nModel fit template:\nglmnet::glmnet(x = missing_arg(), y = missing_arg(), weights = missing_arg(), \n    alpha = 1, family = \"gaussian\")\n\n\nDefine a workflow for Lasso regression.\n\nlasso_wflow &lt;- \n  workflow() |&gt; \n  add_recipe(rec) |&gt; \n  add_model(lasso_model)\n\nThen estimate the model.\n\nmodel_fit &lt;- \n  lasso_wflow |&gt; \n  fit(data = dat1)\n\n\ntidy(model_fit)\n\n\nAttaching package: 'Matrix'\n\n\nThe following objects are masked from 'package:tidyr':\n\n    expand, pack, unpack\n\n\nLoaded glmnet 4.1-8\n\n\n# A tibble: 3 × 3\n  term        estimate penalty\n  &lt;chr&gt;          &lt;dbl&gt;   &lt;dbl&gt;\n1 (Intercept) 6.28e-17       0\n2 x1          2.01e+ 0       0\n3 x2          1.26e- 1       0\n\n\n\n\n3.1.3.2 Set regularization path\nPrevious result is slightly different from a result of typical linear regression.\n\nworkflow() |&gt; \n  add_recipe(rec) |&gt; \n  add_model(linear_reg()) |&gt; \n  fit(data = dat1) |&gt; \n  tidy()\n\n# A tibble: 3 × 5\n  term        estimate std.error statistic p.value\n  &lt;chr&gt;          &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;   &lt;dbl&gt;\n1 (Intercept) 3.37e-18     0.172  1.96e-17 1      \n2 x1          2.02e+ 0     0.306  6.60e+ 0 0.00272\n3 x2          1.34e- 1     0.306  4.38e- 1 0.684  \n\n\nA region is that {glmnet} consumes additional parameter lambda to set a series of values called “regularization path”, and the model approximates between the closet path values. So, if you want to obtain more correct coefficient or prediction associated with a particular penalty amount, include the value to regularization path with glmnet-specific optional parameter path_values in set_engine(). See Technical aspects of the glmnet model\n\nregularization_path &lt;- c(3:0) / (2 * N - 1)\n\nlasso_model &lt;- \n  linear_reg(penalty = 0, mixture = 1) |&gt; \n  set_engine(\"glmnet\", path_values = regularization_path)\n\n\nlasso_model |&gt; \n  translate()\n\nLinear Regression Model Specification (regression)\n\nMain Arguments:\n  penalty = 0\n  mixture = 1\n\nComputational engine: glmnet \n\nModel fit template:\nglmnet::glmnet(x = missing_arg(), y = missing_arg(), weights = missing_arg(), \n    lambda = regularization_path, alpha = 1, family = \"gaussian\")\n\n\nDefine a workflow for Lasso regression.\n\nlasso_wflow &lt;- \n  workflow() |&gt; \n  add_recipe(rec) |&gt; \n  add_model(lasso_model)\n\nmodel_fit &lt;- \n  lasso_wflow |&gt; \n  fit(data = dat1)\n\n\ntidy(model_fit)\n\n# A tibble: 3 × 3\n  term        estimate penalty\n  &lt;chr&gt;          &lt;dbl&gt;   &lt;dbl&gt;\n1 (Intercept) 6.35e-17       0\n2 x1          2.02e+ 0       0\n3 x2          1.33e- 1       0\n\n\nTo obtain coefficients for all four penalty amounts of interest in the regularization path, we can extract the glmnet object and apply specific function.\n\nmodel_fit |&gt; \n  extract_fit_parsnip() |&gt; \n  extract_fit_engine() |&gt; \n  predict(type = \"coefficients\") |&gt; \n  round(4)\n\n3 x 4 sparse Matrix of class \"dgCMatrix\"\n                s0     s1     s2     s3\n(Intercept) 0.0000 0.0000 0.0000 0.0000\nx1          1.8771 1.9279 1.9743 2.0207\nx2          .      0.0409 0.0872 0.1335\n\n\n\n\n\n3.1.4 Ex 3.2: Ridge\nRidge is very similar to Lasso, except setting mixture = 0. Let us define the workflow for ridge regression.\n\nregularization_path &lt;- c(3:0) / (N - 1) * 2\n\nridge_model &lt;- \n  linear_reg(penalty = 0, mixture = 0) |&gt; \n  set_engine(\"glmnet\", path_values = regularization_path)\n\nridge_wflow &lt;- \n  workflow() |&gt; \n  add_recipe(rec) |&gt; \n  add_model(ridge_model)\n\n\nmodel_fit &lt;- \n  ridge_wflow |&gt; \n  fit(data = dat1)\n\nmodel_fit |&gt; \n  extract_fit_parsnip() |&gt; \n  extract_fit_engine() |&gt; \n  predict(type = \"coefficients\") |&gt; \n  round(4)\n\n3 x 4 sparse Matrix of class \"dgCMatrix\"\n                s0     s1     s2     s3\n(Intercept) 0.0000 0.0000 0.0000 0.0000\nx1          1.1178 1.2687 1.5069 2.0194\nx2          0.5667 0.5477 0.4639 0.1345\n\n\nEven though obtaining coefficients is somewhat tedious because it requires extraction of underlying model objects, making a prediction from all the penalty amounts of interest is a little bit more convenient by using multi_predict() function. However, data preprocessing for new data needs to be separately done.\n\nridge_pred &lt;- \n  model_fit |&gt; \n  extract_fit_parsnip() |&gt; \n  multi_predict(\n    new_data = bake(prep(rec, dat1), dat1)[c('x1', 'x2')], \n    penalty = regularization_path\n  )\n\nLet us see prediction results.\n\ndat1 |&gt; \n  bind_cols(ridge_pred) |&gt; \n  unnest(.pred)\n\n# A tibble: 28 × 5\n      x1    x2     y penalty .pred\n   &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;   &lt;dbl&gt; &lt;dbl&gt;\n 1    -6    -3    -3   0     -2.77\n 2    -6    -3    -3   0.333 -2.36\n 3    -6    -3    -3   0.667 -2.11\n 4    -6    -3    -3   1     -1.93\n 5    -4    -1    -2   0     -1.81\n 6    -4    -1    -2   0.333 -1.45\n 7    -4    -1    -2   0.667 -1.26\n 8    -4    -1    -2   1     -1.13\n 9    -3    -2    -1   0     -1.40\n10    -3    -2    -1   0.333 -1.24\n# ℹ 18 more rows",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Regularized regression</span>"
    ]
  }
]