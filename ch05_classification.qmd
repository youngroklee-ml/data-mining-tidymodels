# Classification

```{r}
library(tidyverse)
library(tidymodels)
```

## Example 5.2

### Load data

To estimate classification models, you first need to convert outcome variable to be a factor, if it is a numeric in original data.

```{r}
dat1 <- read_csv("data/ch7_dat1.csv") |> 
  mutate(class = as.factor(class))
dat1
```

### Split data

Let us divide data into two fold: training and testing data. Within tidymodels framework, `{rsamples}` package, a part of `{tidymodels}` provides functions to split data.

`initial_split()` randomly split data into training and testing data, while `initial_time_split()` take first rows of data as training and last rows of data as testing data. To be consistent with a book example, let us use `initial_time_split()` here.

In this example, let us use 8 data points as training data, and just one data point as testing data.

```{r}
dat1_split <- initial_time_split(dat1, prop = 8 / 9)
dat_train <- training(dat1_split)
dat_test <- testing(dat1_split)
```

:::{.callout-note}
A book examples used 7 data points as training data, but we are using 8 here due to a parameter setting mechanism in tidymodels framework. Within tidymodels framework, 8 data points are the minimum number of data points to use 3 neighborhoods. If you use 7 data points as training data, tidymodels will automatically reset the number of neighbors to be 2. For more details, please see [https://parsnip.tidymodels.org/reference/details_nearest_neighbor_kknn.html](https://parsnip.tidymodels.org/reference/details_nearest_neighbor_kknn.html).
:::


### 3-NN model estimation

Tidymodels framework uses `{kknn}` as an engine of k-nearest-neighbor models for both classification and regression. After installing `{kknn}` package, call `nearest_neighbor()` with `neighbors` argument to specify number of neighbors to use in estimation. You must specify mode of the model to be either `"classification"` or `"regression"`. In this example, use `"classification"` mode.

```{r}
knn_model <- 
  nearest_neighbor(neighbors = 3) |> 
  set_engine("kknn", scale = FALSE) |> 
  set_mode("classification")
```

Then, estimate the model with `fit()` function.

```{r}
knn_fit <- 
  knn_model |> 
  fit(class ~ X1 + X2, data = dat_train)
```


Let us check estimated results on training data.

```{r}
dat_train |> 
  bind_cols(
    knn_fit |> 
      extract_fit_engine() |> 
      pluck("fitted.values") |> 
      set_names("estimated_class")
  )
```


### Prediction

Let us make a prediction on testing data, by calling `predict()` function.

```{r}
dat_test |> 
  bind_cols(
    predict(knn_fit, new_data = dat_test)
  )
```


## Examples 5.3 - 5.4

### Load data

```{r}
dat3 <- read_csv("data/ch5_dat3.csv")
dat3
```

### Convert data type

First, convert output to be a factor, because you are going to build a classification model. Please use typical data wrangling instead of using recipe when transforming output variable.

```{r}
dat3 <- 
  dat3 |> 
  mutate(class = as.factor(class))
```

Define a recipe to convert categorical input variable into factors by using recipe. When original variable type is numeric, use `step_num2factor()`. When original variable type is character, use `step_string2factor()`.

```{r}
nb_rec <- 
  recipe(class ~ gender + age_gr, data = dat3) |> 
  step_num2factor(age_gr, levels = c("1", "2", "3", "4")) |> 
  step_string2factor(gender)
```

:::{.callout-note}
If you include output variable type conversion within a recipe and include it in a workflow, `predict()` will return an error because it removes output variable from `new_data` before applying a recipe.
:::

### Ex 5.3: Naive bayes clssification

#### Estimate a classifier

Let us use `parsnip::naive_Bayes()` to define naive bayes model. By default, it uses `{klaR}` engine and requires installation and loading `{discrim}` package to fit the model. Please find more details [here](https://parsnip.tidymodels.org/reference/details_naive_Bayes_klaR.html).

```{r}
library(discrim)

nb_model <- naive_Bayes(smoothness = 0) |> 
  set_engine("klaR") # use {klaR} engine; this is default engine
```

You can check engine-level model fitting function template by calling `translate()` function.

```{r}
nb_model |> translate()
```


Let us define workflow and train the model.

```{r}
nb_fit <- 
  workflow() |> 
  add_recipe(nb_rec) |> 
  add_model(nb_model) |> 
  fit(dat3)

nb_fit
```


#### Prediction

Let us predict posterior probabilities and predict class as well. Both can by done by calling `predict()` function, where you can specify `type = "prob"` for posterior probability prediction and `type = "class"` for class prediction.

```{r}
results <- dat3 |> 
  bind_cols(
    predict(nb_fit, new_data = dat3, type = "prob"),
    predict(nb_fit, new_data = dat3, type = "class")
  )

results
```


#### Use `{naivebayes}` engine

Instead of default engine `{klaR}`, let us use different engine from `{naivebayes}` package. The only change that you need to make is engine name argument in `set_engine()` when defining a model. It provides an identical results.

```{r}
library(discrim)

nb_model <- naive_Bayes(smoothness = 0) |> 
  set_engine("naivebayes") # use {naivebayes} engine

nb_model |> translate()

nb_fit <- 
  workflow() |> 
  add_recipe(nb_rec) |> 
  add_model(nb_model) |> 
  fit(dat3)

nb_fit

results <- dat3 |> 
  bind_cols(
    predict(nb_fit, new_data = dat3, type = "prob"),
    predict(nb_fit, new_data = dat3, type = "class")
  )

results
```


### Ex 5.4: Performance evaluation

`{yardstick}`, which is a part of `{tidymodels}` framework, provides functions to evaluate performance of classification and regression models. Here, let us take a look at several classification performance metrics.


#### Confusion matrix

Create confusion matrix by calling `conf_mat()` with classification result data frame. For other required arguments, `truth` represents a column name of actual class labels and `estimate` represents a column name of predicted class labels.

```{r}
results |> 
  conf_mat(truth = class, estimate = .pred_class)
```

#### Accuracy

Evaluate simple classification accuracy by `accuracy()`

```{r}
results |> 
  accuracy(truth = class, estimate = .pred_class)
```

#### Sensitivity

Use `sens()`. Pass `event_level` argument if needs to explicitly define "event" class label. It argument is only applicable to binary classification, and the value should be either `"first"` or `"second"`.

```{r}
results |> 
  sens(truth = class, estimate = .pred_class, event_level = "first")
```

#### Specificity

Use `spec()`.

```{r}
results |> 
  spec(truth = class, estimate = .pred_class, event_level = "first")
```

#### F1-score

Use `f_meas()`

```{r}
results |> 
  f_meas(truth = class, estimate = .pred_class, event_level = "first")
```

#### Evaluate multiple metrics

`{yardstick}` provides a convenient way to evaluate multiple metrics.

First, create a set of metrics to compute by calling `metric_set()` with metric functions of interest as arguments.

```{r}
multiple_metrics <- metric_set(accuracy, sens, spec, f_meas)
```

Let's print the object.

```{r}
multiple_metrics
```

By looking at classes and a type of `multiple_metrics` object, you can see that it is a callable function.

```{r}
class(multiple_metrics)
typeof(multiple_metrics)
```

Now, call the function to compute multiple metrics of interest.

```{r}
results |> 
  multiple_metrics(truth = class, estimate = .pred_class, event_level = "first")
```

